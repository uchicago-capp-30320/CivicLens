{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CivicLens DOCS","text":"<p>CivicLens is a platform designed to abstract procedures and bureaucracy from the public commenting process. This site serves as internal documentation for current (and future!) contributors to the project.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>CivicLens is currently under active development. This site will continue to be updated to reflect additional features.</p> <p>CivicLens requires <code>poetry</code> (see here for more information) to run. To install the necessary dependencies, run <code>poetry run</code> from the project directory. You can the run the project locally by prefixing python commands with <code>poetry run</code> or by using <code>poetry shell</code> to open activate the virtual environment.</p> <p>To run the test suite, run <code>poetry run pytest</code>.</p>"},{"location":"#set-up-pre-commit-hooks","title":"Set Up Pre-Commit Hooks","text":"<p>To set up pre-commit hooks that lint and test before pushing to the repo, execute the following commands in your command line locally: <code>pip install pre-commit</code> to install the pre-commit library. <code>pre-commit install</code> to create the hooks in your .git/hooks/pre-commit directory.</p>"},{"location":"#set-up-environment-variable-for-api-key","title":"Set Up Environment Variable for API Key","text":"<p>To obtain a regulations.gov API key, request one on the API page</p> <p>For security reasons, we want to use an Environment Variable to store the API key. This repository is set up to access an ignored .env file in the CivicLens folder. To use functions which access the API, within the .env file you should set <code>REG_GOV_API_KEY=</code> to the API key you requested.</p> <p>For testing or temporary purposes, you can also set <code>REG_GOV_API_KEY=\"DEMO_KEY\"</code>. This is a demo functionality built into regulations.gov for very limited API access.</p> <p>We also use the .env file to store relevant sensitive information pertaining to our database and web framework. We extract all of this information in the <code>utils/constants.py</code> file.</p>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#architecture-documentation","title":"Architecture Documentation","text":"<p>CivicLens is built largely on top of the regulations.gov API. In many ways, one can think of this application as a wrapper for this API designed to promote more transparency into rulemaking process and give users better abilities to search for opportunities to comment.</p> <p>At a modular level the application contains:</p> <ul> <li><code>/collect</code>: Contains library with functions to make calls to the regulations.gov API, data pipeline to run nightly chronjobs to collect newly posted data, and SQL files to create the tables that comprise our Postgres database.</li> <li><code>/nlp</code>: Models to generate representative comments, topic modeling, and sentiment analysis. Also holds scripts to run NLP jobs upon new data intake, along with updating the results of the analysis within the database.</li> <li><code>/regulations</code>: Django backend hosting CivicLens. Models, views, and endpoints live here.</li> <li><code>/templates</code>: HTML for the website's pages, along with layouts and other HTML objects (navigational bars, footers).</li> <li><code>/tests</code>: Main testing suite for data intake, updating databases, NLP models, and helper functions.</li> <li><code>/utils</code>: Miscellaneous collection of helper functions and constants.</li> <li><code>/website</code>: Django project folder containing <code>settings.py</code> and other admin code.</li> </ul>"},{"location":"architecture/#data-intake-and-flow","title":"Data Intake and Flow","text":"<p>With the rulemaking and comment process maintained by regulations.gov, CivicLens relies entirely on this API for its data intake. How data is collected, stored, and processed for analysis is diagrammed below.</p> <pre><code>graph LR\n  A[regulations.gov API] --&gt; |Nightly Chronjob| B[ETL];\n  B --&gt; C{Postgres Database};\n  C ----&gt; |New Comment Text| D[NLP Analysis];\n  D --&gt; C;\n  C ----&gt; |Document Data + Analysis| E[Django]\n  E --&gt; F[CivicLens.com];</code></pre> <p>Data is pulled nightly (via chronjob) from regulations.gov. Information on dockets, documents, and comments is collected and stored in a Postgres database (more information on fields can be found in Models). After metadata and comment text from the API is stored, if a sufficient number of new comments are collected a secondary job is triggered to run an NLP analysis of the comment text. This job produces representative comments, plain English document titles, and topic and sentiment analysis describing the comments. These outputs are then seperately uploaded to our database.</p>"},{"location":"collect/","title":"collect","text":"<p>Quick start guide for CivicLens' internal library for accessing data via the regulations.gov API. This library contains methods for collecting comments, dockets, and documents from regulations.gov.</p>"},{"location":"collect/#getting-started","title":"Getting Started","text":"<p>To collect data from regulations.gov, you'll first need an API key which you can request here. Once you have your key, you can add it as an Environment Variable by following the steps described under Contributing, or you can replace <code>api_key</code> with your unique key.</p>"},{"location":"collect/#example-1-retrieving-data-from-one-docket","title":"Example 1 - Retrieving data from one docket","text":"<p>Let's walk through how to collect data for the proposed rule \"FR-6362-P-01 Reducing Barriers to HUD-Assisted Housing.\" You can read the actual rule and view comments at this link. To collect data on this rule, we'll need to search by the Document ID, which we can pass into the <code>params</code> argument like this:</p> <p><pre><code>from access_api_data import pull_reg_gov_data\n\napi_key = \"YOUR-KEY_HERE\"\nsearch_terms = {\"filter[searchTerm]\": \"HUD-2024-0031-0001\"}\n\ndoc = pull_reg_gov_data(\n    api_key,\n    data_type=\"documents\",  # set the datatype to document\n    params=search_terms,\n)\n</code></pre> Now that we've collected the metadata for the document, we can use the documents's object ID to collect all the comments posted to it.</p> <pre><code>doc_object_id = doc[0][\"attributes\"][\"objectId\"]\n\ncomment_data = pull_reg_gov_data(\n    api_key,\n    data_type=\"comments\",\n    params={\"filter[commentOnId]\": doc_object_id},\n)\n</code></pre> <p>Finally, we can get the text for each comment by iterating over the comment metadata and making an additional request for the text.</p> <pre><code>comment_json_text = []\nfor commentId in comment_data[0][\"attributes\"][\"objectId\"]:\n    comment_json_text.append(\n        pull_reg_gov_data(\n            api_key,\n            data_type=\"comments\",\n            params={\"filter[commentOnId]\": commentId},\n        )\n    )\n</code></pre>"},{"location":"collect/#example-2-retrieving-all-comments-made-in-the-first-two-weeks-of-april-2024","title":"Example 2 - Retrieving all comments made in the first two weeks of April 2024","text":"<p>In this example, we demonstrate how to gather all public comments posted within the first ten days of April 2024. This method can also be applied to collect documents or dockets by adjusting 'data_type'. <pre><code>comments_apr_01_10 = pull_reg_gov_data(\n    api_key,\n    data_type=\"comments\",\n    start_date=\"2024-05-01\",\n    end_date=\"2024-05-10\",\n    )\n</code></pre></p>"},{"location":"collect/#reference","title":"Reference","text":""},{"location":"collect/#civiclens.collect.access_api_data","title":"<code>access_api_data</code>","text":"<p>This code pulls heavily from the following existing repositories:</p> <p>https://github.com/willjobs/regulations-public-comments https://github.com/jacobfeldgoise/regulations-comments-downloader</p>"},{"location":"collect/#civiclens.collect.access_api_data.pull_reg_gov_data","title":"<code>pull_reg_gov_data(api_key, data_type, start_date=None, end_date=None, params=None, print_remaining_requests=False, skip_duplicates=False)</code>","text":"<p>Returns the JSON associated with a request to the API; max length of 24000</p> <p>Draws heavily from this [repository] (https://github.com/willjobs/regulations-public-comments/blob/master/ comments_downloader.py)</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>'dockets', 'documents', or 'comments' -- what kind of data we want back from the API</p> required <code>start_date</code> <code>str in YYYY-MM-DD format</code> <p>the inclusive start date of our data pull</p> <code>None</code> <code>end_date</code> <code>str in YYYY-MM-DD format</code> <p>the inclusive end date of our data pull</p> <code>None</code> <code>params</code> <code>dict</code> <p>Parameters to specify to the endpoint request. Defaults to None. If we are querying the non-details endpoint, we also append the \"page[size]\" parameter so that we always get the maximum page size of 250 elements per page.</p> <code>None</code> <code>print_remaining_requests</code> <code>bool</code> <p>Whether to print out the number of remaining requests this hour, based on the response headers. Defaults to False.</p> <code>False</code> <code>wait_for_rate_reset</code> <code>bool</code> <p>Determines whether to wait to re-try if we run out of requests in a given hour. Defaults to False.</p> required <code>skip_duplicates</code> <code>bool</code> <p>If a request returns multiple items when only 1 was expected, should we skip that request? Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>JSON-ified request response</p> Source code in <code>civiclens/collect/access_api_data.py</code> <pre><code>def pull_reg_gov_data(  # noqa: C901,E501\n    api_key,\n    data_type,\n    start_date=None,\n    end_date=None,\n    params=None,\n    print_remaining_requests=False,\n    skip_duplicates=False,\n):\n    \"\"\"\n    Returns the JSON associated with a request to the API; max length of 24000\n\n    Draws heavily from this [repository]\n    (https://github.com/willjobs/regulations-public-comments/blob/master/\n    comments_downloader.py)\n\n    Args:\n        data_type (str): 'dockets', 'documents', or 'comments' -- what kind of\n            data we want back from the API\n        start_date (str in YYYY-MM-DD format, optional): the inclusive start\n            date of our data pull\n        end_date (str in YYYY-MM-DD format, optional): the inclusive end date\n            of our data pull\n        params (dict, optional): Parameters to specify to the endpoint request.\n            Defaults to None.\n            If we are querying the non-details endpoint, we also append the\n            \"page[size]\" parameter so that we always get the maximum page size\n            of 250 elements per page.\n        print_remaining_requests (bool, optional): Whether to print out the\n            number of remaining requests this hour, based on the response\n            headers.\n            Defaults to False.\n        wait_for_rate_reset (bool, optional): Determines whether to wait to\n            re-try if we run out of requests in a given hour. Defaults to\n            False.\n        skip_duplicates (bool, optional): If a request returns multiple items\n            when only 1 was expected, should we skip that request? Defaults to\n            False.\n\n    Returns:\n        dict: JSON-ified request response\n    \"\"\"\n    # generate the right API request\n    api_url = \"https://api.regulations.gov/v4/\"\n    endpoint = f\"{api_url}{data_type}\"\n    params = params if params is not None else {}\n\n    # Our API key has a rate limit of 1,000 requests/hour.\n    # If we hit that limit, we can\n    # retry every WAIT_MINUTES minutes (more frequently than once an hour, in\n    # case our request limit\n    # is updated sooner). We will sleep for POLL_SECONDS seconds at a time to\n    # see if we've been\n    # interrupted. Otherwise we'd have to wait a while before getting\n    # interrupted. We could do this\n    # with threads, but that gets more complicated than it needs to be.\n    STATUS_CODE_OVER_RATE_LIMIT = 429\n    WAIT_SECONDS = 3600  # Default to 1 hour\n\n    # if any dates are specified, format those and add to the params\n    if start_date or end_date:\n        param_dates = api_date_format_params(start_date, end_date)\n        params.update(param_dates)\n\n    # whether we are querying the search endpoint (e.g., /documents)\n    # or the \"details\" endpoint\n    if endpoint.split(\"/\")[-1] in [\"dockets\", \"documents\", \"comments\"]:\n        params = {**params, \"page[size]\": 250}  # always get max page size\n\n    # Rather than do requests.get(), use this approach to (attempt to)\n    # gracefully handle noisy connections to the server\n    # We sometimes get SSL errors (unexpected EOF or ECONNRESET), so this\n    # should hopefully help us retry.\n    session = requests.Session()\n    session.mount(\"https\", HTTPAdapter(max_retries=4))\n\n    def poll_for_response(api_key, wait_for_rate_reset):\n        r = session.get(\n            endpoint, headers={\"X-Api-Key\": api_key}, params=params, verify=True\n        )\n\n        if r.status_code == 200:\n            # SUCCESS! Return the JSON of the request\n            num_requests_left = int(r.headers[\"X-RateLimit-Remaining\"])\n            if (\n                print_remaining_requests\n                or (num_requests_left &lt; 10)\n                or (num_requests_left &lt;= 100 and num_requests_left % 10 == 0)\n                or (num_requests_left % 100 == 0 and num_requests_left &lt; 1000)\n            ):\n                print(f\"(Requests left: {r.headers['X-RateLimit-Remaining']})\")\n\n            return [True, r.json()]\n        else:\n            if (\n                r.status_code == STATUS_CODE_OVER_RATE_LIMIT\n                and wait_for_rate_reset\n            ):\n                the_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                retry_after = r.headers.get(\"Retry-After\", None)\n                wait_time = (\n                    int(retry_after)\n                    if retry_after and retry_after.isdigit()\n                    else WAIT_SECONDS\n                )\n                print(\n                    f\"\"\"Rate limit exceeded at {the_time}.\n                    Waiting {wait_time} seconds to retry.\"\"\"\n                )\n                time.sleep(wait_time)\n            elif _is_duplicated_on_server(r.json()) and skip_duplicates:\n                print(\"****Duplicate entries on server. Skipping.\")\n                print(r.json()[\"errors\"][0][\"detail\"])\n            else:  # some other kind of error\n                print([r, r.status_code])\n                print(r.json())\n                r.raise_for_status()\n\n        return [False, r.json()]\n\n    if data_type == \"comments\" or data_type == \"documents\":\n        all_objects = []\n        unique_objects = {}\n\n        params.update(\n            {\n                \"page[size]\": 250,\n                \"sort\": \"lastModifiedDate,documentId\",\n            }\n        )\n\n        last_modified_date = None\n        continue_fetching = True\n\n        while continue_fetching:\n            success, r_json = poll_for_response(\n                api_key, wait_for_rate_reset=True\n            )\n            if success:\n                all_objects.extend(r_json[\"data\"])\n                print(\n                    f\"\"\"Fetched {len(r_json['data'])} objects,\n                    total: {len(all_objects)}\"\"\"\n                )\n\n                # Check and handle the pagination\n                has_next_page = r_json[\"meta\"].get(\"hasNextPage\", False)\n                print(f\"Has next page: {has_next_page}\")\n\n                if len(r_json[\"data\"]) &lt; 250 or not has_next_page:\n                    print(\n                        \"\"\"No more pages or fewer than 250\n                        comments fetched, stopping...\"\"\"\n                    )\n                    continue_fetching = False\n                else:\n                    last_modified_date = format_datetime_for_api(\n                        r_json[\"data\"][-1][\"attributes\"][\"lastModifiedDate\"]\n                    )\n                    params.update(\n                        {\n                            \"filter[lastModifiedDate][ge]\": last_modified_date,\n                            \"page[size]\": 250,\n                            \"sort\": \"lastModifiedDate,documentId\",\n                            \"page[number]\": 1,\n                            \"api_key\": api_key,\n                        }\n                    )\n                    if end_date:\n                        params.update(\n                            {\n                                \"filter[lastModifiedDate][le]\": f\"{end_date} 23:59:59\"  # noqa: E501\n                            }\n                        )\n                    print(f\"Fetching more data from {last_modified_date}\")\n            else:\n                print(\"Failed to fetch data\")\n                continue_fetching = False\n\n        # Remove Duplicates\n        for obj in all_objects:\n            unique_objects[obj[\"id\"]] = obj\n        return list(unique_objects.values())\n\n    else:\n        doc_data = None  # Initialize doc_data to None\n        for i in range(1, 21):  # Fetch up to 20 pages\n            params.update(\n                {\n                    \"page[size]\": 250,\n                    # Ensure that only lastModifiedDate is considered,\n                    # dockets cant take in documentID\n                    \"sort\": \"lastModifiedDate\",\n                    \"page[number]\": str(i),\n                    \"api_key\": api_key,\n                }\n            )\n\n            success, r_json = poll_for_response(\n                api_key, wait_for_rate_reset=True\n            )\n\n            if success or (\n                _is_duplicated_on_server(r_json) and skip_duplicates\n            ):\n                if doc_data is not None:\n                    doc_data += r_json[\"data\"]\n                else:\n                    doc_data = r_json[\"data\"]\n\n            # Break if it's the last page\n            if r_json[\"meta\"][\"lastPage\"]:\n                return doc_data\n\n    raise RuntimeError(f\"Unrecoverable error; {r_json}\")\n</code></pre>"},{"location":"collect/#civiclens.collect.access_api_data.api_date_format_params","title":"<code>api_date_format_params(start_date=None, end_date=None)</code>","text":"<p>Formats dates to be passed to API call. Assumes we want whole days, and aren't filtering by time.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str in YYYY-MM-DD format</code> <p>the inclusive start date of our data pull</p> <code>None</code> <code>end_date</code> <code>str in YYYY-MM-DD format</code> <p>the inclusive end date of our data pull</p> <code>None</code> <p>Returns:</p> Name Type Description <code>date_param</code> <code>dict</code> <p>dict containing the right formatted date calls</p> Source code in <code>civiclens/collect/access_api_data.py</code> <pre><code>def api_date_format_params(start_date=None, end_date=None):\n    \"\"\"\n    Formats dates to be passed to API call. Assumes we want whole days, and\n    aren't filtering by time.\n\n    Args:\n        start_date (str in YYYY-MM-DD format, optional): the inclusive start\n            date of our data pull\n        end_date (str in YYYY-MM-DD format, optional): the inclusive end date\n            of our data pull\n\n    Returns:\n        date_param (dict): dict containing the right formatted date calls\n    \"\"\"\n    date_param = {}\n    if start_date is not None:\n        date_param.update(\n            {\"filter[lastModifiedDate][ge]\": f\"{start_date} 00:00:00\"}\n        )\n        print(f\"{start_date=}\")\n    if end_date is not None:\n        date_param.update(\n            {\"filter[lastModifiedDate][le]\": f\"{end_date} 23:59:59\"}\n        )\n        print(f\"{end_date=}\")\n\n    return date_param\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database","title":"<code>move_data_from_api_to_database</code>","text":""},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.add_comments_based_on_comment_date_range","title":"<code>add_comments_based_on_comment_date_range(start_date, end_date)</code>","text":"<p>Add comments to the comments table based on a date range of when the comments were posted</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>the date in YYYY-MM-DD format to pull data from (inclusive)</p> required <code>end_date</code> <code>str</code> <p>the date in YYYY-MM-DD format to stop the data pull (inclusive)</p> required <p>Returns: nothing; adds comments, if available, to the db</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def add_comments_based_on_comment_date_range(\n    start_date: str, end_date: str\n) -&gt; None:\n    \"\"\"\n    Add comments to the comments table based on a date range of when the\n    comments were posted\n\n    Args:\n        start_date (str): the date in YYYY-MM-DD format to pull data from\n            (inclusive)\n        end_date (str): the date in YYYY-MM-DD format to stop the data pull\n            (inclusive)\n\n    Returns: nothing; adds comments, if available, to the db\n    \"\"\"\n    comment_data = pull_reg_gov_data(\n        REG_GOV_API_KEY,\n        \"comments\",\n        start_date=start_date,\n        end_date=end_date,\n    )\n\n    for comment in comment_data:\n        logging.info(f\"processing comment {comment['id']} \")\n\n        all_comment_data = merge_comment_text_and_data(REG_GOV_API_KEY, comment)\n\n        document_id = all_comment_data[\"data\"][\"attributes\"].get(\n            \"commentOnDocumentId\", \"\"\n        )\n\n        logging.info(f\"checking {document_id} is in the db\")\n\n        if verify_database_existence(\n            \"regulations_document\",\n            document_id,\n            \"id\",\n        ):\n            logging.info(\n                f\"{document_id} is in the db! begin processing comment\"\n            )\n\n            # clean\n            clean_comment_data(all_comment_data)\n\n            # qa\n            qa_comment_data(all_comment_data)\n\n            # insert\n            insert_response = insert_comment_into_db(all_comment_data)\n            if insert_response[\"error\"]:\n                logging.error(insert_response[\"description\"])\n            else:\n                logging.info(\n                    f\"added comment {all_comment_data['id']} to the db\"\n                )\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.add_comments_to_db","title":"<code>add_comments_to_db(doc_list, print_statements=True)</code>","text":"<p>Add comments on a list of documents to the database</p> <p>Parameters:</p> Name Type Description Default <code>doc_list</code> <code>list of json objects</code> <p>what is returned from an API call for documents</p> required <code>print_statements</code> <code>boolean</code> <p>whether to print info on progress</p> <code>True</code> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def add_comments_to_db(\n    doc_list: list[dict], print_statements: bool = True\n) -&gt; None:\n    \"\"\"\n    Add comments on a list of documents to the database\n\n    Args:\n        doc_list (list of json objects): what is returned from an API call for\n            documents\n        print_statements (boolean): whether to print info on progress\n\n    \"\"\"\n    for doc in doc_list:\n        document_id = doc[\"id\"]\n        document_object_id = doc[\"attributes\"][\"objectId\"]\n        commentable = doc[\"attributes\"][\"openForComment\"]\n        # get the comments, comment text, and add to db\n        if commentable:\n            if not verify_database_existence(\n                \"regulations_comment\", document_id, \"document_id\"\n            ):  # doc doesn't exist in the db; it's new\n                if print_statements:\n                    logging.info(\n                        f\"\"\"no comments found in database for document\n                        {document_id}\"\"\"\n                    )\n\n                add_comments_to_db_for_new_doc(document_object_id)\n\n                if print_statements:\n                    logging.info(\n                        f\"\"\"tried to add comments on document\n                        {document_id} to the db\"\"\"\n                    )\n\n            else:  # doc exists in db; only need to add new comments\n                add_comments_to_db_for_existing_doc(\n                    document_id, document_object_id, print_statements\n                )\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.add_comments_to_db_for_existing_doc","title":"<code>add_comments_to_db_for_existing_doc(document_id, document_object_id, print_statements=True)</code>","text":"<p>Gets the most recent comment in the comments table and pulls comments for     a doc from the API since then</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>id (str</code> <p>the id for the document we want comments for (comes from the document json object)</p> required <code>document_object_id</code> <code>str</code> <p>the object id for the document we want comments for (comes from the document json object)</p> required <code>print_statements</code> <code>bool</code> <p>whether to print during, default is true</p> <code>True</code> <p>Returns: nothing; adds comments, if available, to the db</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def add_comments_to_db_for_existing_doc(\n    document_id: str, document_object_id: str, print_statements: bool = True\n) -&gt; None:\n    \"\"\"\n    Gets the most recent comment in the comments table and pulls comments for\n        a doc from the API since then\n\n    Args:\n        document id (str): the id for the document we want comments for (comes\n            from the document json object)\n        document_object_id (str): the object id for the document we want\n            comments for (comes from the document json object)\n        print_statements (bool): whether to print during, default is true\n\n    Returns: nothing; adds comments, if available, to the db\n    \"\"\"\n\n    # get date of most recent comment on this doc in the db\n    most_recent_comment_date = get_most_recent_doc_comment_date(document_id)\n    if print_statements:\n        logging.info(\n            f\"\"\"comments found for document {document_id}, most recent was\n            {most_recent_comment_date}\"\"\"\n        )\n\n    comment_data = pull_reg_gov_data(\n        REG_GOV_API_KEY,\n        \"comments\",\n        params={\n            \"filter[commentOnId]\": document_object_id,\n            \"filter[lastModifiedDate][ge]\": most_recent_comment_date,\n        },\n    )\n\n    for comment in comment_data:\n        all_comment_data = merge_comment_text_and_data(REG_GOV_API_KEY, comment)\n\n        # if documumrnent is not in the db, add it\n        document_id = all_comment_data.get(\"commentOnDocumentId\", \"\")\n        if not verify_database_existence(\n            \"regulations_document\", document_id, \"id\"\n        ):\n            # call the API to get the document info\n            doc_list = pull_reg_gov_data(\n                REG_GOV_API_KEY,\n                \"documents\",\n                params={\"filter[objectId]\": document_id},\n            )\n            add_documents_to_db(doc_list)\n\n        # clean\n        clean_comment_data(all_comment_data)\n\n        # qa\n        qa_comment_data(all_comment_data)\n\n        insert_response = insert_comment_into_db(all_comment_data)\n        if insert_response[\"error\"]:\n            logging.error(insert_response[\"description\"])\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.add_comments_to_db_for_new_doc","title":"<code>add_comments_to_db_for_new_doc(document_object_id)</code>","text":"<p>Add comments to the comments table for a new doc (ie, when we have just     added the doc to the database)</p> <p>Parameters:</p> Name Type Description Default <code>document_object_id</code> <code>str</code> <p>the object id for the document we want comments for (comes from the document json object)</p> required <p>Returns: nothing; adds comments, if available, to the db</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def add_comments_to_db_for_new_doc(document_object_id: str) -&gt; None:\n    \"\"\"\n    Add comments to the comments table for a new doc (ie, when we have just\n        added the doc to the database)\n\n    Args:\n        document_object_id (str): the object id for the document we want\n            comments for (comes from the document json object)\n\n    Returns: nothing; adds comments, if available, to the db\n    \"\"\"\n    comment_data = pull_reg_gov_data(\n        REG_GOV_API_KEY,\n        \"comments\",\n        params={\"filter[commentOnId]\": document_object_id},\n    )\n    # add comment data to comments table in the database\n    for comment in comment_data:\n        all_comment_data = merge_comment_text_and_data(REG_GOV_API_KEY, comment)\n\n        # clean\n        clean_comment_data(all_comment_data)\n\n        # qa\n        qa_comment_data(all_comment_data)\n\n        insert_response = insert_comment_into_db(all_comment_data)\n\n        if insert_response[\"error\"]:\n            logging.error(insert_response[\"description\"])\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.add_data_quality_flag","title":"<code>add_data_quality_flag(data_id, data_type, error_message)</code>","text":"<p>Add data to the regulations_dataqa table is qa assert statements flag</p> <p>Parameters:</p> Name Type Description Default <code>data_id</code> <code>str</code> <p>id field of the data in question</p> required <code>data_type</code> <code>str</code> <p>whether docket, document, or comment</p> required <code>error_message</code> <code>Error</code> <p>the error and message raised by the assert statement</p> required <p>Returns: nothing, add a row to regulations_dataqa</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def add_data_quality_flag(\n    data_id: str, data_type: str, error_message: str\n) -&gt; None:\n    \"\"\"Add data to the regulations_dataqa table is qa assert statements flag\n\n    Args:\n        data_id (str): id field of the data in question\n        data_type (str): whether docket, document, or comment\n        error_message (Error): the error and message raised by the assert\n            statement\n    Returns: nothing, add a row to regulations_dataqa\n    \"\"\"\n    connection, cursor = connect_db_and_get_cursor()\n    with connection:\n        with cursor:\n            cursor.execute(\n                \"\"\"INSERT INTO regulations_dataqa (\n                    \"data_id\",\n                    \"data_type\",\n                    \"error_message\",\n                    \"added_at\"\n                ) VALUES (%s, %s, %s, %s)\n                ON CONFLICT (id) DO UPDATE SET\n                    data_id = EXCLUDED.data_id,\n                    data_type = EXCLUDED.data_type,\n                    error_message = EXCLUDED.error_message,\n                    added_at = EXCLUDED.added_at;\"\"\",\n                (data_id, data_type, str(error_message), datetime.now()),\n            )\n        connection.commit()\n\n    if connection:\n        cursor.close()\n        connection.close()\n\n    logging.info(f\"Added data quality flag for {data_id} of type {data_type}.\")\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.add_dockets_to_db","title":"<code>add_dockets_to_db(doc_list, print_statements=True)</code>","text":"<p>Add the dockets connected to a list of documents into the database</p> <p>Parameters:</p> Name Type Description Default <code>doc_list</code> <code>list of json objects</code> <p>what is returned from an API call for documents</p> required <code>print_statements</code> <code>boolean</code> <p>whether to print info on progress</p> <code>True</code> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def add_dockets_to_db(\n    doc_list: list[dict], print_statements: bool = True\n) -&gt; None:\n    \"\"\"\n    Add the dockets connected to a list of documents into the database\n\n    Args:\n        doc_list (list of json objects): what is returned from an API call\n            for documents\n        print_statements (boolean): whether to print info on progress\n\n    \"\"\"\n    for doc in doc_list:\n        docket_id = doc[\"attributes\"][\"docketId\"]\n        commentable = doc[\"attributes\"][\"openForComment\"]\n        # if docket not in db, add it\n        if commentable and (\n            not verify_database_existence(\"regulations_docket\", docket_id)\n        ):\n            docket_data = pull_reg_gov_data(\n                REG_GOV_API_KEY,\n                \"dockets\",\n                params={\"filter[searchTerm]\": docket_id},\n            )\n\n            # clean\n            clean_docket_data(docket_data)\n\n            # qa\n            qa_docket_data(docket_data)\n\n            # add docket_data to docket table in the database\n            insert_response = insert_docket_into_db(docket_data)\n            if insert_response[\"error\"]:\n                logging.error(insert_response[\"description\"])\n            else:\n                if print_statements:\n                    logging.info(f\"Added docket {docket_id} to the db\")\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.add_documents_to_db","title":"<code>add_documents_to_db(doc_list, print_statements=True)</code>","text":"<p>Add a list of document json objects into the database</p> <p>Parameters:</p> Name Type Description Default <code>doc_list</code> <code>list of json objects</code> <p>what is returned from an API call for documents</p> required <code>print_statements</code> <code>boolean</code> <p>whether to print info on progress</p> <code>True</code> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def add_documents_to_db(\n    doc_list: list[dict], print_statements: bool = True\n) -&gt; None:\n    \"\"\"\n    Add a list of document json objects into the database\n\n    Args:\n        doc_list (list of json objects): what is returned from an API call for\n            documents\n        print_statements (boolean): whether to print info on progress\n\n    \"\"\"\n    for doc in doc_list:\n        document_id = doc[\"id\"]\n        commentable = doc[\"attributes\"][\"openForComment\"]\n        if commentable and (\n            not verify_database_existence(\"regulations_document\", document_id)\n        ):\n            # add this doc to the documents table in the database\n            full_doc_info = query_register_API_and_merge_document_data(doc)\n            # qa step\n            qa_document_data(full_doc_info)\n\n            # clean step\n            clean_document_data(full_doc_info)\n\n            # insert step\n            insert_response = insert_document_into_db(full_doc_info)\n            if insert_response[\"error\"]:\n                logging.info(insert_response[\"description\"])\n                # would want to add logging here\n            else:\n                if print_statements:\n                    logging.info(f\"added document {document_id} to the db\")\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.check_CFR_data","title":"<code>check_CFR_data(document_data)</code>","text":"<p>Check that the CFR field looks right</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def check_CFR_data(document_data: json) -&gt; bool:\n    \"\"\"\n    Check that the CFR field looks right\n    \"\"\"\n    try:\n        assert (\n            document_data[\"CFR\"] is None\n            or document_data[\"CFR\"] == \"Not Found\"\n            or \"CFR\" in document_data[\"CFR\"]\n            or document_data[\"CFR\"].isalpha()\n        )\n        return True\n    except AssertionError:\n        return False\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.clean_comment_data","title":"<code>clean_comment_data(comment_data)</code>","text":"<p>Clean comment text -- make sure dates are formatted correctly</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def clean_comment_data(comment_data: json) -&gt; None:\n    \"\"\"\n    Clean comment text -- make sure dates are formatted correctly\n    \"\"\"\n\n    comment_text_attributes = comment_data[\"data\"][\"attributes\"]\n\n    # format the date fields\n    for date_field in [\"modifyDate\", \"postedDate\", \"receiveDate\"]:\n        comment_text_attributes[date_field] = (\n            datetime.strptime(\n                comment_text_attributes[date_field], \"%Y-%m-%dT%H:%M:%SZ\"\n            )\n            if comment_text_attributes[date_field]\n            else None\n        )\n\n    # clean the text\n    if comment_text_attributes[\"comment\"] is not None:\n        comment_text_attributes[\"comment\"] = clean_text(\n            comment_text_attributes[\"comment\"]\n        )\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.clean_document_data","title":"<code>clean_document_data(document_data)</code>","text":"<p>Clean document data in place; run cleaning code on summary</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def clean_document_data(document_data: json) -&gt; None:\n    \"\"\"\n    Clean document data in place; run cleaning code on summary\n    \"\"\"\n    if document_data[\"summary\"] is not None:\n        document_data[\"summary\"] = clean_text(document_data[\"summary\"])\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.connect_db_and_get_cursor","title":"<code>connect_db_and_get_cursor()</code>","text":"<p>Connect to the CivicLens database and return the objects</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def connect_db_and_get_cursor() -&gt; (\n    tuple[psycopg2.extensions.connection, psycopg2.extensions.cursor]\n):\n    \"\"\"\n    Connect to the CivicLens database and return the objects\n    \"\"\"\n    connection = psycopg2.connect(\n        database=DATABASE_NAME,\n        user=DATABASE_USER,\n        password=DATABASE_PASSWORD,\n        host=DATABASE_HOST,\n        port=DATABASE_PORT,\n    )\n    cursor = connection.cursor()\n    return connection, cursor\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.extract_xml_text_from_doc","title":"<code>extract_xml_text_from_doc(doc)</code>","text":"<p>Take a document's json object, pull the xml text, add the text to the object</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>json</code> <p>the object from regulations.gov API</p> required <p>Returns:</p> Name Type Description <code>processed_data</code> <code>json</code> <p>the object with added text</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def extract_xml_text_from_doc(doc: json) -&gt; json:\n    \"\"\"\n    Take a document's json object, pull the xml text, add the text to the\n    object\n\n    Args:\n        doc (json): the object from regulations.gov API\n\n    Returns:\n        processed_data (json): the object with added text\n    \"\"\"\n    processed_data = []\n\n    if not doc:\n        return processed_data\n\n    fr_doc_num = doc[\"attributes\"][\"frDocNum\"]\n    if fr_doc_num:\n        xml_url = fetch_fr_document_details(fr_doc_num)\n        xml_content = fetch_xml_content(xml_url)\n        if xml_content:\n            extracted_data = parse_xml_content(xml_content)\n            processed_data.append(\n                {**doc, **extracted_data}\n            )  # merge the json objects\n\n    return processed_data\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.fetch_fr_document_details","title":"<code>fetch_fr_document_details(fr_doc_num)</code>","text":"<p>Retrieves xml url for document text from federal register.</p> <p>Parameters:</p> Name Type Description Default <code>fr_doc_num</code> <code>str</code> <p>the unique id (comes from regulations.gov api info)</p> required <p>Returns:</p> Type Description <code>str</code> <p>xml url (str)</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def fetch_fr_document_details(fr_doc_num: str) -&gt; str:\n    \"\"\"\n    Retrieves xml url for document text from federal register.\n\n    Args:\n        fr_doc_num (str): the unique id (comes from regulations.gov api info)\n\n    Returns:\n        xml url (str)\n    \"\"\"\n    api_endpoint = f\"https://www.federalregister.gov/api/v1/documents/{fr_doc_num}.json?fields[]=full_text_xml_url\"  # noqa: E231\n    response = requests.get(api_endpoint)\n    if response.status_code == 200:\n        data = response.json()\n        return data.get(\"full_text_xml_url\")\n    else:\n        error_message = f\"Error fetching FR document details for {fr_doc_num}: {response.status_code}\"  # noqa: E501\n        raise Exception(error_message)\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.fetch_xml_content","title":"<code>fetch_xml_content(url)</code>","text":"<p>Fetches XML content from a given URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>the xml url that we want to retrive text from</p> required <p>Returns:</p> Type Description <code>str</code> <p>response.text (str): the text</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def fetch_xml_content(url: str) -&gt; str:\n    \"\"\"\n    Fetches XML content from a given URL.\n\n    Args:\n        url (str): the xml url that we want to retrive text from\n\n    Returns:\n        response.text (str): the text\n    \"\"\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.text\n    else:\n        error_message = (\n            f\"Error fetching XML content from {url}: {response.status_code}\"\n        )\n        raise Exception(error_message)\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.get_comment_text","title":"<code>get_comment_text(api_key, comment_id)</code>","text":"<p>Get the text of a comment, with retry logic to handle rate limits.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>key for the regulations.gov API</p> required <code>comment_id</code> <code>str</code> <p>the id for the comment</p> required <p>Returns:</p> Type Description <code>dict</code> <p>json object for the comment text</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def get_comment_text(api_key: str, comment_id: str) -&gt; dict:\n    \"\"\"\n    Get the text of a comment, with retry logic to handle rate limits.\n\n    Args:\n        api_key (str): key for the regulations.gov API\n        comment_id (str): the id for the comment\n\n    Returns:\n        json object for the comment text\n    \"\"\"\n    api_url = \"https://api.regulations.gov/v4/comments/\"\n    endpoint = f\"{api_url}{comment_id}?include=attachments\"\n\n    STATUS_CODE_OVER_RATE_LIMIT = 429\n    WAIT_SECONDS = 3600  # Default to 1 hour\n\n    session = requests.Session()\n    session.mount(\"https\", HTTPAdapter(max_retries=4))\n\n    while True:\n        response = session.get(\n            endpoint, headers={\"X-Api-Key\": api_key}, verify=True\n        )\n\n        if response.status_code == 200:\n            # SUCCESS! Return the JSON of the request\n            return response.json()\n        elif response.status_code == STATUS_CODE_OVER_RATE_LIMIT:\n            retry_after = response.headers.get(\"Retry-After\", None)\n            wait_time = (\n                int(retry_after)\n                if retry_after and retry_after.isdigit()\n                else WAIT_SECONDS\n            )\n            the_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            logging.info(\n                f\"\"\"Rate limit exceeded at {the_time}.\n                Waiting {wait_time} seconds to retry.\"\"\"\n            )\n            time.sleep(wait_time)\n        else:\n            logging.info(\n                f\"\"\"Failed to retrieve comment data.\n                Status code: {response.status_code}\"\"\"\n            )\n            return None\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.get_most_recent_doc_comment_date","title":"<code>get_most_recent_doc_comment_date(doc_id)</code>","text":"<p>Returns the date of the most recent comment for a doc</p> <p>Parameters:</p> Name Type Description Default <code>doc_id</code> <code>str</code> <p>the regulations.gov doc id</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>datetime</code> <p>the most recent date</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def get_most_recent_doc_comment_date(doc_id: str) -&gt; str:\n    \"\"\"\n    Returns the date of the most recent comment for a doc\n\n    Args:\n        doc_id (str): the regulations.gov doc id\n\n    Returns:\n        response (datetime): the most recent date\n    \"\"\"\n    connection, cursor = connect_db_and_get_cursor()\n    with connection:\n        with cursor:\n            query = f\"\"\"SELECT MAX(\"posted_date\")\n                FROM regulations_comment\n                WHERE \"document_id\" = '{doc_id}';\"\"\"  # noqa: E231, E702\n            cursor.execute(query)\n            response = cursor.fetchall()\n\n    if connection:\n        cursor.close()\n        connection.close()\n\n    # format the text\n    # it seems that the regulations.gov API returns postedDate rounded to the\n    # hour. If we used that naively as the most recent date, we might miss some\n    # comments when we pull comments again. By backing up one hour, we trade\n    # off some unnecessary API calls for ensuring we don't miss anything\n    date_dt = response[0][0]\n    one_hour_prior = date_dt - dt.timedelta(hours=1)\n    most_recent_date = one_hour_prior.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    return most_recent_date\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.insert_comment_into_db","title":"<code>insert_comment_into_db(comment_data)</code>","text":"<p>Insert the info on a comment into the PublicComments table</p> <p>Parameters:</p> Name Type Description Default <code>comment_data</code> <code>json</code> <p>the comment info from regulations.gov API</p> required <p>Returns:</p> Type Description <code>dict</code> <p>nothing unless an error; adds the info into the table</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def insert_comment_into_db(comment_data: json) -&gt; dict:\n    \"\"\"\n    Insert the info on a comment into the PublicComments table\n\n    Args:\n        comment_data (json): the comment info from regulations.gov API\n\n    Returns:\n        nothing unless an error; adds the info into the table\n    \"\"\"\n    connection, cursor = connect_db_and_get_cursor()\n\n    attributes = comment_data[\"attributes\"]\n    comment_text_attributes = comment_data[\"data\"][\"attributes\"]\n\n    # Map JSON attributes to corresponding table columns\n    comment_id = comment_data[\"id\"]\n    objectId = attributes.get(\"objectId\", \"\")\n    commentOn = comment_text_attributes.get(\"commentOn\", \"\")\n    document_id = comment_text_attributes.get(\"commentOnDocumentId\", \"\")\n    duplicateComments = comment_text_attributes.get(\"duplicateComments\", 0)\n    stateProvinceRegion = comment_text_attributes.get(\"stateProvinceRegion\", \"\")\n    subtype = comment_text_attributes.get(\"subtype\", \"\")\n    comment = comment_text_attributes.get(\"comment\", \"\")\n    firstName = comment_text_attributes.get(\"firstName\", \"\")\n    lastName = comment_text_attributes.get(\"lastName\", \"\")\n    address1 = comment_text_attributes.get(\"address1\", \"\")\n    address2 = comment_text_attributes.get(\"address2\", \"\")\n    city = comment_text_attributes.get(\"city\", \"\")\n    category = comment_text_attributes.get(\"category\", \"\")\n    country = comment_text_attributes.get(\"country\", \"\")\n    email = comment_text_attributes.get(\"email\", \"\")\n    phone = comment_text_attributes.get(\"phone\", \"\")\n    govAgency = comment_text_attributes.get(\"govAgency\", \"\")\n    govAgencyType = comment_text_attributes.get(\"govAgencyType\", \"\")\n    organization = comment_text_attributes.get(\"organization\", \"\")\n    originalDocumentId = comment_text_attributes.get(\"originalDocumentId\", \"\")\n    modifyDate = comment_text_attributes.get(\"modifyDate\", \"\")\n    pageCount = comment_text_attributes.get(\"pageCount\", 0)\n    postedDate = comment_text_attributes.get(\"postedDate\", \"\")\n    receiveDate = comment_text_attributes.get(\"receiveDate\", \"\")\n    title = attributes.get(\"title\", \"\")\n    trackingNbr = comment_text_attributes.get(\"trackingNbr\", \"\")\n    withdrawn = comment_text_attributes.get(\"withdrawn\", False)\n    reasonWithdrawn = comment_text_attributes.get(\"reasonWithdrawn\", \"\")\n    zip = comment_text_attributes.get(\"zip\", \"\")\n    restrictReason = comment_text_attributes.get(\"restrictReason\", \"\")\n    restrictReasonType = comment_text_attributes.get(\"restrictReasonType\", \"\")\n    submitterRep = comment_text_attributes.get(\"submitterRep\", \"\")\n    submitterRepAddress = comment_text_attributes.get(\"submitterRepAddress\", \"\")\n    submitterRepCityState = comment_text_attributes.get(\n        \"submitterRepCityState\", \"\"\n    )\n\n    # SQL INSERT statement\n    query = \"\"\"\n    INSERT INTO regulations_comment (\n        \"id\",\n        \"object_id\",\n        \"comment_on\",\n        \"document_id\",\n        \"duplicate_comments\",\n        \"state_province_region\",\n        \"subtype\",\n        \"comment\",\n        \"first_name\",\n        \"last_name\",\n        \"address1\",\n        \"address2\",\n        \"city\",\n        \"category\",\n        \"country\",\n        \"email\",\n        \"phone\",\n        \"gov_agency\",\n        \"gov_agency_type\",\n        \"organization\",\n        \"original_document_id\",\n        \"modify_date\",\n        \"page_count\",\n        \"posted_date\",\n        \"receive_date\",\n        \"title\",\n        \"tracking_nbr\",\n        \"withdrawn\",\n        \"reason_withdrawn\",\n        \"zip\",\n        \"restrict_reason\",\n        \"restrict_reason_type\",\n        \"submitter_rep\",\n        \"submitter_rep_address\",\n        \"submitter_rep_city_state\"\n    ) VALUES (\n        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s\n    )\n    ON CONFLICT (id) DO UPDATE SET\n        object_id = EXCLUDED.object_id,\n        comment_on = EXCLUDED.comment_on,\n        document_id = EXCLUDED.document_id,\n        duplicate_comments = EXCLUDED.duplicate_comments,\n        state_province_region = EXCLUDED.state_province_region,\n        subtype = EXCLUDED.subtype,\n        comment = EXCLUDED.comment,\n        first_name = EXCLUDED.first_name,\n        last_name = EXCLUDED.last_name,\n        address1 = EXCLUDED.address1,\n        address2 = EXCLUDED.address2,\n        city = EXCLUDED.city,\n        category = EXCLUDED.category,\n        country = EXCLUDED.country,\n        email = EXCLUDED.email,\n        phone = EXCLUDED.phone,\n        gov_agency = EXCLUDED.gov_agency,\n        gov_agency_type = EXCLUDED.gov_agency_type,\n        organization = EXCLUDED.organization,\n        original_document_id = EXCLUDED.original_document_id,\n        modify_date = EXCLUDED.modify_date,\n        page_count = EXCLUDED.page_count,\n        posted_date = EXCLUDED.posted_date,\n        receive_date = EXCLUDED.receive_date,\n        title = EXCLUDED.title,\n        tracking_nbr = EXCLUDED.tracking_nbr,\n        withdrawn = EXCLUDED.withdrawn,\n        reason_withdrawn = EXCLUDED.reason_withdrawn,\n        zip = EXCLUDED.zip,\n        restrict_reason = EXCLUDED.restrict_reason,\n        restrict_reason_type = EXCLUDED.restrict_reason_type,\n        submitter_rep = EXCLUDED.submitter_rep,\n        submitter_rep_address = EXCLUDED.submitter_rep_address,\n        submitter_rep_city_state = EXCLUDED.submitter_rep_city_state;\n    \"\"\"\n\n    # Execute the SQL statement\n    try:\n        with connection.cursor() as cursor:\n            cursor.execute(\n                query,\n                (\n                    comment_id,\n                    objectId,\n                    commentOn,\n                    document_id,\n                    duplicateComments,\n                    stateProvinceRegion,\n                    subtype,\n                    comment,\n                    firstName,\n                    lastName,\n                    address1,\n                    address2,\n                    city,\n                    category,\n                    country,\n                    email,\n                    phone,\n                    govAgency,\n                    govAgencyType,\n                    organization,\n                    originalDocumentId,\n                    modifyDate,\n                    pageCount,\n                    postedDate,\n                    receiveDate,\n                    title,\n                    trackingNbr,\n                    withdrawn,\n                    reasonWithdrawn,\n                    zip,\n                    restrictReason,\n                    restrictReasonType,\n                    submitterRep,\n                    submitterRepAddress,\n                    submitterRepCityState,\n                ),\n            )\n            connection.commit()\n\n        if connection:\n            connection.close()\n\n    except Exception as e:\n        error_message = f\"\"\"Error inserting comment {comment_data['id']} into\n        comment table: {e}\"\"\"\n        logging.error(error_message)\n        return {\n            \"error\": True,\n            \"message\": e,\n            \"description\": error_message,\n        }\n\n    return {\n        \"error\": False,\n        \"message\": None,\n        \"description\": None,\n    }\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.insert_docket_into_db","title":"<code>insert_docket_into_db(docket_data)</code>","text":"<p>Run assert statements to check docket data looks right</p> <p>Parameters:</p> Name Type Description Default <code>docket_data</code> <code>json</code> <p>the docket info from regulations.gov API</p> required <p>Returns: nothing unless an error; adds the info into the table</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def insert_docket_into_db(docket_data: json) -&gt; dict:\n    \"\"\"\n    Run assert statements to check docket data looks right\n\n    Args:\n        docket_data (json): the docket info from regulations.gov API\n\n    Returns: nothing unless an error; adds the info into the table\n    \"\"\"\n\n    data_for_db = docket_data[0]\n    attributes = data_for_db[\"attributes\"]\n    try:\n        connection, cursor = connect_db_and_get_cursor()\n\n        with connection:\n            with cursor:\n                cursor.execute(\n                    \"\"\"\n                    INSERT INTO regulations_docket (\n                        \"id\",\n                        \"docket_type\",\n                        \"last_modified_date\",\n                        \"agency_id\",\n                        \"title\",\n                        \"object_id\",\n                        \"highlighted_content\"\n                    ) VALUES (\n                        %s, %s, %s, %s, %s, %s, %s\n                    )\n                    ON CONFLICT (id) DO UPDATE SET\n                        docket_type = EXCLUDED.docket_type,\n                        last_modified_date = EXCLUDED.last_modified_date,\n                        agency_id = EXCLUDED.agency_id,\n                        title = EXCLUDED.title,\n                        object_id = EXCLUDED.object_id,\n                        highlighted_content = EXCLUDED.highlighted_content;\n                    \"\"\",\n                    (\n                        data_for_db[\"id\"],\n                        attributes[\"docketType\"],\n                        attributes[\"lastModifiedDate\"],\n                        attributes[\"agencyId\"],\n                        attributes[\"title\"],\n                        attributes[\"objectId\"],\n                        attributes[\"highlightedContent\"],\n                    ),\n                )\n                connection.commit()\n    except Exception as e:\n        error_message = f\"\"\"Error inserting docket {data_for_db[\"id\"]}\n        into dockets table: {e}\"\"\"\n        logging.error(error_message)\n        return {\n            \"error\": True,\n            \"message\": e,\n            \"description\": error_message,\n        }\n\n    return {\n        \"error\": False,\n        \"message\": None,\n        \"description\": None,\n    }\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.insert_document_into_db","title":"<code>insert_document_into_db(document_data)</code>","text":"<p>Insert the info on a document into the documents table</p> <p>Parameters:</p> Name Type Description Default <code>document_data</code> <code>json</code> <p>the document info from regulations.gov API</p> required <p>Returns:</p> Type Description <code>dict</code> <p>nothing unless an error; adds the info into the table</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def insert_document_into_db(document_data: json) -&gt; dict:\n    \"\"\"\n    Insert the info on a document into the documents table\n\n    Args:\n        document_data (json): the document info from regulations.gov API\n\n    Returns:\n        nothing unless an error; adds the info into the table\n    \"\"\"\n    attributes = document_data[\"attributes\"]\n\n    fields_to_insert = (\n        document_data[\"id\"],\n        attributes[\"documentType\"],\n        attributes[\"lastModifiedDate\"],\n        attributes[\"frDocNum\"],\n        attributes[\"withdrawn\"],\n        attributes[\"agencyId\"],\n        attributes[\"commentEndDate\"],\n        attributes[\"postedDate\"],\n        attributes[\"docketId\"],\n        attributes[\"subtype\"],\n        attributes[\"commentStartDate\"],\n        attributes[\"openForComment\"],\n        attributes[\"objectId\"],\n        document_data[\"links\"][\"self\"],\n        document_data[\"agencyType\"],\n        document_data[\"CFR\"],\n        document_data[\"RIN\"],\n        attributes[\"title\"],\n        document_data[\"summary\"],\n        document_data[\"dates\"],\n        document_data[\"furtherInformation\"],\n        document_data[\"supplementaryInformation\"],\n    )\n\n    query = \"\"\"INSERT INTO regulations_document (\"id\",\n                            \"document_type\",\n                            \"last_modified_date\",\n                            \"fr_doc_num\",\n                            \"withdrawn\",\n                            \"agency_id\",\n                            \"comment_end_date\",\n                            \"posted_date\",\n                            \"docket_id\",\n                            \"subtype\",\n                            \"comment_start_date\",\n                            \"open_for_comment\",\n                            \"object_id\",\n                            \"full_text_xml_url\",\n                            \"agency_type\",\n                            \"cfr\",\n                            \"rin\",\n                            \"title\",\n                            \"summary\",\n                            \"dates\",\n                            \"further_information\",\n                            \"supplementary_information\") \\\n                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n                        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n                    ON CONFLICT (id) DO UPDATE SET\n                        document_type = EXCLUDED.document_type,\n                        last_modified_date = EXCLUDED.last_modified_date,\n                        fr_doc_num = EXCLUDED.fr_doc_num,\n                        withdrawn = EXCLUDED.withdrawn,\n                        agency_id = EXCLUDED.agency_id,\n                        comment_end_date = EXCLUDED.comment_end_date,\n                        posted_date = EXCLUDED.posted_date,\n                        docket_id = EXCLUDED.docket_id,\n                        subtype = EXCLUDED.subtype,\n                        comment_start_date = EXCLUDED.comment_start_date,\n                        open_for_comment = EXCLUDED.open_for_comment,\n                        object_id = EXCLUDED.object_id,\n                        full_text_xml_url = EXCLUDED.full_text_xml_url,\n                        agency_type = EXCLUDED.agency_type,\n                        cfr = EXCLUDED.cfr,\n                        rin = EXCLUDED.rin,\n                        title = EXCLUDED.title,\n                        summary = EXCLUDED.summary,\n                        dates = EXCLUDED.dates,\n                        further_information = EXCLUDED.further_information,\n                        supplementary_information =\n                        EXCLUDED.supplementary_information;\"\"\"\n    try:\n        connection, cursor = connect_db_and_get_cursor()\n        with connection:\n            with cursor:\n                cursor.execute(\n                    query,\n                    fields_to_insert,\n                )\n                connection.commit()\n        if connection:\n            cursor.close()\n            connection.close()\n\n    except Exception as e:\n        error_message = f\"\"\"Error inserting document\n        {document_data['id']} into dockets table: {e}\"\"\"\n        logging.error(error_message)\n        return {\n            \"error\": True,\n            \"message\": e,\n            \"description\": error_message,\n        }\n\n    return {\n        \"error\": False,\n        \"message\": None,\n        \"description\": None,\n    }\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.merge_comment_text_and_data","title":"<code>merge_comment_text_and_data(api_key, comment_data)</code>","text":"<p>Combine comment json object with the comment text json object</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>key for the regulations.gov API</p> required <code>comment_data</code> <code>json</code> <p>the json object from regulations.gov</p> required <p>Returns:</p> Type Description <code>json</code> <p>combined json object for the comment and text</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def merge_comment_text_and_data(api_key: str, comment_data: json) -&gt; json:\n    \"\"\"\n    Combine comment json object with the comment text json object\n\n    Args:\n        api_key (str): key for the regulations.gov API\n        comment_data (json): the json object from regulations.gov\n\n    Returns:\n        combined json object for the comment and text\n    \"\"\"\n\n    comment_text_data = get_comment_text(api_key, comment_data[\"id\"])\n\n    # DECISION: only track the comment data; don't include the info on comment\n    # attachments which is found elsewhere in comment_text_data\n\n    all_comment_data = {**comment_data, **comment_text_data}\n    return all_comment_data\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.parse_xml_content","title":"<code>parse_xml_content(xml_content)</code>","text":"<p>Parses XML content and extracts relevant data such as agency type, CFR, RIN, title, summary, etc.</p> <p>Parameters:</p> Name Type Description Default <code>xml_content</code> <code>str</code> <p>xml formatted text</p> required <p>Returns:</p> Name Type Description <code>extracted_data</code> <code>dict</code> <p>contains key parts of the extracted text</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def parse_xml_content(xml_content: str) -&gt; dict:\n    \"\"\"\n    Parses XML content and extracts relevant data such as agency type, CFR,\n    RIN, title, summary, etc.\n\n    Args:\n        xml_content (str): xml formatted text\n\n    Returns:\n        extracted_data (dict): contains key parts of the extracted text\n    \"\"\"\n    # Convert the XML string to an ElementTree object\n    root = ET.fromstring(xml_content)\n\n    # Initialize a dictionary to hold extracted data\n    extracted_data = {}\n\n    # Extract Agency Type\n    agency_type = root.find('.//AGENCY[@TYPE=\"S\"]')\n    extracted_data[\"agencyType\"] = (\n        agency_type.text if agency_type is not None else \"Not Found\"\n    )\n\n    # Extract CFR\n    cfr = root.find(\".//CFR\")\n    extracted_data[\"CFR\"] = cfr.text if cfr is not None else \"Not Found\"\n\n    # Extract RIN\n    rin = root.find(\".//RIN\")\n    extracted_data[\"RIN\"] = rin.text if rin is not None else \"Not Found\"\n\n    # Extract Title (Subject)\n    title = root.find(\".//SUBJECT\")\n    extracted_data[\"title\"] = title.text if title is not None else \"Not Found\"\n\n    # Extract Summary\n    summary = root.find(\".//SUM/P\")\n    extracted_data[\"summary\"] = (\n        summary.text if summary is not None else \"Not Found\"\n    )\n\n    # Extract DATES\n    dates = root.find(\".//DATES/P\")\n    extracted_data[\"dates\"] = dates.text if dates is not None else \"Not Found\"\n\n    # Extract Further Information\n    furinf = root.find(\".//FURINF/P\")\n    extracted_data[\"furtherInformation\"] = (\n        furinf.text if furinf is not None else \"Not Found\"\n    )\n\n    # Extract Supplementary Information\n    supl_info_texts = []\n    supl_info_elements = root.findall(\".//SUPLINF/*\")\n    for element in supl_info_elements:\n        # Assuming we want to gather all text from children tags within\n        # &lt;SUPLINF&gt;\n        if element.text is not None:\n            supl_info_texts.append(element.text.strip())\n        for sub_element in element:\n            if sub_element.text is not None:\n                supl_info_texts.append(sub_element.text.strip())\n\n    # Join all pieces of supplementary information text into a single string\n    extracted_data[\"supplementaryInformation\"] = \" \".join(supl_info_texts)\n\n    return extracted_data\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.pull_all_api_data_for_date_range","title":"<code>pull_all_api_data_for_date_range(start_date, end_date, pull_dockets, pull_documents, pull_comments)</code>","text":"<p>Pull different types of data from regulations.gov API based on date range</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>the date in YYYY-MM-DD format to pull data from (inclusive)</p> required <code>end_date</code> <code>str</code> <p>the date in YYYY-MM-DD format to stop the data pull (inclusive)</p> required <code>pull_dockets</code> <code>boolean</code> required <p>Returns:</p> Type Description <code>None</code> <p>None; adds data to the db</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def pull_all_api_data_for_date_range(\n    start_date: str,\n    end_date: str,\n    pull_dockets: bool,\n    pull_documents: bool,\n    pull_comments: bool,\n) -&gt; None:\n    \"\"\"\n    Pull different types of data from regulations.gov API based on date range\n\n    Args:\n        start_date (str): the date in YYYY-MM-DD format to pull data from\n            (inclusive)\n        end_date (str): the date in YYYY-MM-DD format to stop the data pull\n            (inclusive)\n        pull_dockets (boolean):\n\n    Returns:\n        None; adds data to the db\n    \"\"\"\n\n    # get documents\n    logging.info(\"getting list of documents within date range\")\n    doc_list = pull_reg_gov_data(\n        REG_GOV_API_KEY,\n        \"documents\",\n        start_date=start_date,\n        end_date=end_date,\n    )\n    logging.info(f\"got {len(doc_list)} documents\")\n    logging.info(\"now extracting docs open for comments from that list\")\n\n    # pull the commentable docs from that list\n    commentable_docs = []\n    for doc in doc_list:\n        docket_id = doc[\"attributes\"][\"docketId\"]\n        if docket_id and doc[\"attributes\"][\"openForComment\"]:\n            commentable_docs.append(doc)\n            # can't add this doc to the db right now because docket needs to be\n            # added first\n            # the db needs the docket primary key first\n\n    logging.info(f\"{len(commentable_docs)} documents open for comment\")\n\n    if pull_dockets:\n        logging.info(\n            \"getting dockets associated with the documents open for comment\"\n        )\n        add_dockets_to_db(commentable_docs)\n        logging.info(\"no more dockets to add to db\")\n\n    if pull_documents:\n        logging.info(\"adding documents to the db\")\n        add_documents_to_db(commentable_docs)\n        logging.info(\"no more documents to add to db\")\n\n    if pull_comments:\n        logging.info(\"adding comments to the db\")\n        add_comments_based_on_comment_date_range(start_date, end_date)\n        logging.info(\"no more comments to add to db\")\n\n    logging.info(\"process finished\")\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.qa_comment_data","title":"<code>qa_comment_data(comment_data)</code>","text":"<p>Run assert statements to check comment data looks right</p> <p>Parameters:</p> Name Type Description Default <code>comment_data</code> <code>json object</code> <p>the document data from the API</p> required <p>Returns: (bool) whether data is in the expected format</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def qa_comment_data(comment_data: json) -&gt; None:\n    \"\"\"\n    Run assert statements to check comment data looks right\n\n    Args:\n        comment_data (json object): the document data from the API\n\n    Returns: (bool) whether data is in the expected format\n    \"\"\"\n\n    attributes = comment_data[\"attributes\"]\n    comment_text_attributes = comment_data[\"data\"][\"attributes\"]\n    try:\n        assert len(comment_data[\"id\"]) &lt; 255, \"id is more than 255 characters\"\n\n        assert (\n            attributes[\"objectId\"][:2] == \"09\"\n        ), \"objectId does not start with '09'\"\n        assert (\n            comment_text_attributes[\"commentOn\"][:2] == \"09\"\n        ), \"commentOn does not start with '09'\"\n        # comment_data[\"commentOnDocumentId\"]\n        assert (\n            comment_text_attributes[\"duplicateComments\"] == 0\n        ), \"duplicateComments != 0\"\n        # assert comment_data[\"stateProvinceRegion\"]\n        assert comment_text_attributes[\n            \"subtype\"\n        ] is None or comment_text_attributes[\"subtype\"] in [\n            \"Public Comment\",\n            \"Comment(s)\",\n        ], \"subtype is not an expected value\"\n        assert isinstance(\n            comment_text_attributes[\"comment\"], str\n        ), \"comment is not string\"\n        # comment_data[\"firstName\"]\n        # comment_data[\"lastName\"]\n        # comment_data[\"address1\"]\n        # comment_data[\"address2\"]\n        # comment_data[\"city\"]\n        # comment_data[\"category\"]\n        # comment_data[\"country\"]\n        # comment_data[\"email\"]\n        # comment_data[\"phone\"]\n        # comment_data[\"govAgency\"]\n        # comment_data[\"govAgencyType\"]\n        # comment_data[\"organization\"]\n        # comment_data[\"originalDocumentId\"]\n        assert isinstance(\n            comment_text_attributes[\"modifyDate\"], datetime\n        ), \"modifyDate is not datetime\"\n        # comment_data[\"pageCount\"]\n        assert isinstance(\n            comment_text_attributes[\"postedDate\"], datetime\n        ), \"postedDate is not datetime\"\n        assert isinstance(\n            comment_text_attributes[\"receiveDate\"], datetime\n        ), \"receiveDate is not datetime\"\n        # comment_data[\"trackingNbr\"]\n        assert (\n            comment_text_attributes[\"withdrawn\"] is False\n        ), \"withdrawn is not False\"\n        # comment_data[\"reasonWithdrawn\"]\n        # comment_data[\"zip\"]\n        # comment_data[\"restrictReason\"]\n        # comment_data[\"restrictReasonType\"]\n        # comment_data[\"submitterRep\"]\n        # comment_data[\"submitterRepAddress\"]\n        # comment_data[\"submitterRepCityState\"]\n\n    except AssertionError as e:\n        id = comment_data[\"id\"]\n        logging.error(f\"AssertionError: comment {id} -- {e}\")\n        add_data_quality_flag(id, \"comment\", e)\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.qa_docket_data","title":"<code>qa_docket_data(docket_data)</code>","text":"<p>Run assert statements to check docket data looks right</p> <p>Parameters:</p> Name Type Description Default <code>docket_data</code> <code>json object</code> <p>the docket data from the API</p> required <p>Returns: (bool) whether data is in the expected format</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def qa_docket_data(docket_data: json) -&gt; None:\n    \"\"\"\n    Run assert statements to check docket data looks right\n\n    Args:\n        docket_data (json object): the docket data from the API\n\n    Returns: (bool) whether data is in the expected format\n    \"\"\"\n\n    attributes = docket_data[0][\"attributes\"]\n    data_for_db = docket_data[0]\n\n    try:\n        # need to check that docket_data is in the right format\n        assert (\n            isinstance(docket_data, list) or len(docket_data) &lt; 1\n        ), \"docket data in wrong format\"\n\n        assert \"attributes\" in data_for_db, \"'attributes' not in docket_data\"\n\n        # check the fields\n        assert (\n            len(data_for_db[\"id\"]) &lt; 255\n        ), \"id field longer than 255 characters\"\n        assert attributes[\"docketType\"] in [\n            \"Rulemaking\",\n            \"Nonrulemaking\",\n        ], \"docketType unexpected value\"\n        assert (\n            len(attributes[\"lastModifiedDate\"]) == 20\n            and \"202\" in attributes[\"lastModifiedDate\"]\n        ), \"lastModifiedDate is unexpected length\"\n        assert attributes[\"agencyId\"].isalpha(), \"agencyId is not just letter\"\n        assert isinstance(attributes[\"title\"], str), \"title is not string\"\n        assert (\n            attributes[\"objectId\"][:2] == \"0b\"\n        ), \"objectId does not start with '0b'\"\n        # attributes[\"highlightedContent\"]\n\n    except AssertionError as e:\n        id = data_for_db[\"id\"]\n        logging.error(f\"AssertionError: docket {id} -- {e}\")\n        add_data_quality_flag(id, \"docket\", e)\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.qa_document_data","title":"<code>qa_document_data(document_data)</code>","text":"<p>Run assert statements to check document data looks right</p> <p>Parameters:</p> Name Type Description Default <code>document_data</code> <code>json object</code> <p>the document data from the API</p> required <p>Returns: (bool) whether data is in the expected format</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def qa_document_data(document_data: json) -&gt; True:\n    \"\"\"\n    Run assert statements to check document data looks right\n\n    Args:\n        document_data (json object): the document data from the API\n\n    Returns: (bool) whether data is in the expected format\n    \"\"\"\n\n    attributes = document_data[\"attributes\"]\n\n    try:\n        assert len(document_data[\"id\"]) &lt; 255\n        assert attributes[\"documentType\"] in [\n            \"Proposed Rule\",\n            \"Other\",\n            \"Notice\",\n            \"Not Found\",\n            \"Rule\",\n        ]\n        # attributes[\"lastModifiedDate\"]\n        assert validate_fr_doc_num(\n            attributes[\"frDocNum\"]\n        ), \"frDocNum contains unexpected characters or is None\"\n        assert attributes[\"withdrawn\"] is False, \"withdrawn is True\"\n        # attributes[\"agencyId\"]\n        assert (\n            len((attributes[\"commentEndDate\"])) == 20\n            and \"202\" in attributes[\"commentEndDate\"]\n        ), \"commentEndDate is unexpected length\"\n        assert (\n            len(attributes[\"postedDate\"]) == 20\n        ), \"postedDate is unexpected length\"\n        # attributes[\"docketId\"]\n        # attributes[\"subtype\"]\n        assert (\n            len(attributes[\"commentStartDate\"]) == 20\n            and \"202\" in attributes[\"commentStartDate\"]\n        ), \"commentStartDate is expected length\"\n        assert attributes[\"openForComment\"] is True, \"openForComment is False\"\n        # attributes[\"objectId\"]\n        assert (\n            \"https\" in document_data[\"links\"][\"self\"]\n        ), \"'https' is not in document_data['links']['self']\"\n        assert (\n            \".gov\" in document_data[\"links\"][\"self\"]\n        ), \"'.gov' is not in document_data['links']['self']\"\n        # document_data[\"agencyType\"]\n        assert check_CFR_data(document_data), \"CFR is not alpha characters\"\n        # document_data[\"RIN\"]\n        assert isinstance(attributes[\"title\"], str), \"title is not string\"\n        assert document_data[\"summary\"] is None or isinstance(\n            document_data[\"summary\"], str\n        ), \"summary is not string\"\n        # document_data[\"dates\"]\n        # document_data[\"furtherInformation\"]\n        assert document_data[\"supplementaryInformation\"] is None or isinstance(\n            document_data[\"supplementaryInformation\"], str\n        ), \"supplementaryInformation is not string\"\n\n    except AssertionError as e:\n        id = document_data[\"id\"]\n        logging.error(f\"AssertionError: document {id} -- {e}\")\n        add_data_quality_flag(id, \"document\", e)\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.query_register_API_and_merge_document_data","title":"<code>query_register_API_and_merge_document_data(doc)</code>","text":"<p>Attempts to pull document text via federal register API and merge with reg gov API data</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>json</code> <p>the raw json for a document from regulations.gov API</p> required <p>Returns:</p> Name Type Description <code>merged_doc</code> <code>json</code> <p>the json with fields for text from federal register API</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def query_register_API_and_merge_document_data(doc: json) -&gt; json:\n    \"\"\"\n    Attempts to pull document text via federal register API and merge with reg\n    gov API data\n\n    Args:\n        doc (json): the raw json for a document from regulations.gov API\n\n    Returns:\n        merged_doc (json): the json with fields for text from federal register\n            API\n    \"\"\"\n\n    # extract the document text using the general register API\n    fr_doc_num = doc.get(\"attributes\", {}).get(\"frDocNum\")\n    document_id = doc[\"id\"]\n    if fr_doc_num:\n        try:\n            xml_url = fetch_fr_document_details(fr_doc_num)\n            xml_content = fetch_xml_content(xml_url)\n            parsed_xml_content = parse_xml_content(xml_content)\n            doc.update(parsed_xml_content)  # merge the json objects\n        except Exception:\n            # if there's an error, that means we can't use the xml_url to get\n            # the doc text, so we enter None for those fields\n            logging.error(\n                rf\"\"\"Error accessing federal register xml data for frDocNum\n                {fr_doc_num}, \\ document id {document_id}\"\"\"\n            )\n            blank_xml_fields = {\n                \"agencyType\": None,\n                \"CFR\": None,\n                \"RIN\": None,\n                \"title\": None,\n                \"summary\": None,\n                \"dates\": None,\n                \"furtherInformation\": None,\n                \"supplementaryInformation\": None,\n            }\n            doc.update(blank_xml_fields)  # merge the json objects\n\n    else:\n        blank_xml_fields = {\n            \"agencyType\": None,\n            \"CFR\": None,\n            \"RIN\": None,\n            \"title\": None,\n            \"summary\": None,\n            \"dates\": None,\n            \"furtherInformation\": None,\n            \"supplementaryInformation\": None,\n        }\n        doc.update(blank_xml_fields)  # merge the json objects\n\n    return doc\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.validate_fr_doc_num","title":"<code>validate_fr_doc_num(field_value)</code>","text":"<p>Check the fr_doc_num field is in the right format</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def validate_fr_doc_num(field_value):\n    \"\"\"\n    Check the fr_doc_num field is in the right format\n    \"\"\"\n\n    # this is a decision: we accept None as a value\n    if field_value is None:\n        return True\n\n    if field_value == \"Not Found\":\n        return True\n\n    if all(char.isdigit() or char == \"-\" for char in field_value):\n        return True\n\n    # else\n    return False\n</code></pre>"},{"location":"collect/#civiclens.collect.move_data_from_api_to_database.verify_database_existence","title":"<code>verify_database_existence(table, api_field_val, db_field='id')</code>","text":"<p>Confirm a row exists in a db table for a given id</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>one of the tables in the CivicLens db</p> required <code>api_field_val</code> <code>str</code> <p>the value we're looking for in the table</p> required <code>db_field</code> <code>str</code> <p>the field in the table where we're looking for the</p> <code>'id'</code> <p>Returns:</p> Type Description <code>bool</code> <p>boolean indicating the value was found</p> Source code in <code>civiclens/collect/move_data_from_api_to_database.py</code> <pre><code>def verify_database_existence(\n    table: str, api_field_val: str, db_field: str = \"id\"\n) -&gt; bool:\n    \"\"\"\n    Confirm a row exists in a db table for a given id\n\n    Args:\n        table (str): one of the tables in the CivicLens db\n        api_field_val (str): the value we're looking for in the table\n        db_field (str): the field in the table where we're looking for the\n        value\n\n    Returns:\n        boolean indicating the value was found\n    \"\"\"\n    connection, cursor = connect_db_and_get_cursor()\n    with connection:\n        with cursor:\n            query = f\"SELECT * FROM {table} WHERE {db_field} = %s;\"  # noqa: E231, E702\n            cursor.execute(query, (api_field_val,))\n            response = cursor.fetchall()\n\n    if connection:\n        cursor.close()\n        connection.close()\n\n    return bool(response)\n</code></pre>"},{"location":"collect/#civiclens.collect.bulk_dl","title":"<code>bulk_dl</code>","text":""},{"location":"collect/#civiclens.collect.bulk_dl.BulkDl","title":"<code>BulkDl</code>","text":"Source code in <code>civiclens/collect/bulk_dl.py</code> <pre><code>class BulkDl:\n    def __init__(self, api_key):\n        \"\"\"\n        Initializes the BulkDl class.\n\n        Args:\n            api_key (str): API key for authenticating requests to the\n            regulations.gov API.\n\n        Attributes:\n            api_key (str): Stored API key for requests.\n            base_url (str): Base URL for the API endpoint.\n            headers (dict): Headers to include in API requests, containing API\n                key and content type.\n            agencies (list[str]): List of agency identifiers (aggregated from\n                https://www.regulations.gov/agencies) to be used in API calls.\n        \"\"\"\n        self.api_key = api_key\n        self.base_url = \"https://api.regulations.gov/v4\"\n        self.headers = {\n            \"X-Api-Key\": self.api_key,\n            \"Content-Type\": \"application/json\",\n        }\n        self.agencies = [\n            \"AID\",\n            \"ATBCB\",\n            \"CFPB\",\n            \"CNCS\",\n            \"COLC\",\n            \"CPPBSD\",\n            \"CPSC\",\n            \"CSB\",\n            \"DHS\",\n            \"DOC\",\n            \"DOD\",\n            \"DOE\",\n            \"DOI\",\n            \"DOJ\",\n            \"DOL\",\n            \"DOS\",\n            \"DOT\",\n            \"ED\",\n            \"EEOC\",\n            \"EIB\",\n            \"EOP\",\n            \"EPA\",\n            \"FFIEC\",\n            \"FMC\",\n            \"FRTIB\",\n            \"FTC\",\n            \"GAPFAC\",\n            \"GSA\",\n            \"HHS\",\n            \"HUD\",\n            \"NARA\",\n            \"NASA\",\n            \"NCUA\",\n            \"NLRB\",\n            \"NRC\",\n            \"NSF\",\n            \"NTSB\",\n            \"ONCD\",\n            \"OPM\",\n            \"PBGC\",\n            \"PCLOB\",\n            \"SBA\",\n            \"SSA\",\n            \"TREAS\",\n            \"USC\",\n            \"USDA\",\n            \"VA\",\n            \"ACF\",\n            \"ACL\",\n            \"AHRQ\",\n            \"AID\",\n            \"AMS\",\n            \"AOA\",\n            \"APHIS\",\n            \"ARS\",\n            \"ASC\",\n            \"ATBCB\",\n            \"ATF\",\n            \"ATR\",\n            \"ATSDR\",\n            \"BIA\",\n            \"BIS\",\n            \"BLM\",\n            \"BLS\",\n            \"BOEM\",\n            \"BOP\",\n            \"BOR\",\n            \"BPA\",\n            \"BPD\",\n            \"BSC\",\n            \"BSEE\",\n            \"CCC\",\n            \"CDC\",\n            \"CDFI\",\n            \"CEQ\",\n            \"CFPB\",\n            \"CISA\",\n            \"CMS\",\n            \"CNCS\",\n            \"COE\",\n            \"COLC\",\n            \"CPPBSD\",\n            \"CPSC\",\n            \"CSB\",\n            \"CSEO\",\n            \"CSREES\",\n            \"DARS\",\n            \"DEA\",\n            \"DEPO\",\n            \"DHS\",\n            \"DOC\",\n            \"DOD\",\n            \"DOE\",\n            \"DOI\",\n            \"DOJ\",\n            \"DOL\",\n            \"DOS\",\n            \"DOT\",\n            \"EAB\",\n            \"EAC\",\n            \"EBSA\",\n            \"ECAB\",\n            \"ECSA\",\n            \"ED\",\n            \"EDA\",\n            \"EEOC\",\n            \"EERE\",\n            \"EIA\",\n            \"EIB\",\n            \"EOA\",\n            \"EOIR\",\n            \"EPA\",\n            \"ERS\",\n            \"ESA\",\n            \"ETA\",\n            \"FAA\",\n            \"FAR\",\n            \"FAS\",\n            \"FBI\",\n            \"FCIC\",\n            \"FCSC\",\n            \"FDA\",\n            \"FEMA\",\n            \"FFIEC\",\n            \"FHWA\",\n            \"FINCEN\",\n            \"FIRSTNET\",\n            \"FISCAL\",\n            \"FLETC\",\n            \"FMC\",\n            \"FMCSA\",\n            \"FNS\",\n            \"FPAC\",\n            \"FRA\",\n            \"FRTIB\",\n            \"FS\",\n            \"FSA\",\n            \"FSIS\",\n            \"FSOC\",\n            \"FTA\",\n            \"FTC\",\n            \"FTZB\",\n            \"FWS\",\n            \"GAPFAC\",\n            \"GIPSA\",\n            \"GSA\",\n            \"HHS\",\n            \"HHSIG\",\n            \"HRSA\",\n            \"HUD\",\n            \"IACB\",\n            \"ICEB\",\n            \"IHS\",\n            \"IPEC\",\n            \"IRS\",\n            \"ISOO\",\n            \"ITA\",\n            \"LMSO\",\n            \"MARAD\",\n            \"MBDA\",\n            \"MMS\",\n            \"MSHA\",\n            \"NAL\",\n            \"NARA\",\n            \"NASA\",\n            \"NASS\",\n            \"NCS\",\n            \"NCUA\",\n            \"NEO\",\n            \"NHTSA\",\n            \"NIC\",\n            \"NIFA\",\n            \"NIGC\",\n            \"NIH\",\n            \"NIST\",\n            \"NLRB\",\n            \"NNSA\",\n            \"NOAA\",\n            \"NPS\",\n            \"NRC\",\n            \"NRCS\",\n            \"NSF\",\n            \"NSPC\",\n            \"NTIA\",\n            \"NTSB\",\n            \"OCC\",\n            \"OEPNU\",\n            \"OFAC\",\n            \"OFCCP\",\n            \"OFPP\",\n            \"OFR\",\n            \"OJJDP\",\n            \"OJP\",\n            \"OMB\",\n            \"ONCD\",\n            \"ONDCP\",\n            \"ONRR\",\n            \"OPM\",\n            \"OPPM\",\n            \"OSHA\",\n            \"OSM\",\n            \"OSTP\",\n            \"OTS\",\n            \"PBGC\",\n            \"PCLOB\",\n            \"PHMSA\",\n            \"PTO\",\n            \"RBS\",\n            \"RHS\",\n            \"RITA\",\n            \"RMA\",\n            \"RTB\",\n            \"RUS\",\n            \"SAMHSA\",\n            \"SBA\",\n            \"SEPA\",\n            \"SLSDC\",\n            \"SSA\",\n            \"SWPA\",\n            \"TA\",\n            \"TREAS\",\n            \"TSA\",\n            \"TTB\",\n            \"USA\",\n            \"USAF\",\n            \"USBC\",\n            \"USCBP\",\n            \"USCG\",\n            \"USCIS\",\n            \"USDA\",\n            \"USDAIG\",\n            \"USGS\",\n            \"USMINT\",\n            \"USN\",\n            \"USPC\",\n            \"USTR\",\n            \"VA\",\n            \"VETS\",\n            \"WAPA\",\n            \"WCPO\",\n            \"WHD\",\n        ]\n\n    def fetch_all_pages(self, endpoint, params, max_items_per_page=250):\n        \"\"\"\n        Iterates through all the pages of API data for a given endpoint\n        until there are no more pages to fetch (occurs at 20 pages, or\n        5000 items).\n\n        Args:\n            endpoint (str): The API endpoint to fetch data from.\n                            ['dockets', 'documents', 'comments']\n            params (dict): Dictionary of parameters to send in the API request.\n            max_items_per_page (int): Maximum number of items per page to\n            request. Max (default) = 250.\n\n        Returns:\n            list: A list of items (dictionaries) fetched from all pages of the\n                API endpoint.\n        \"\"\"\n        items = []\n        page = 1\n        while True:\n            params[\"page[number]\"] = page\n            params[\"page[size]\"] = max_items_per_page\n\n            response = requests.get(\n                f\"{self.base_url}/{endpoint}\",\n                headers=self.headers,\n                params=params,\n            )\n            print(f\"Requesting: {response.url}\")\n\n            if response.status_code == 200:\n                try:\n                    data = response.json()\n                except ValueError:\n                    print(\"Failed to decode JSON response\")\n                    break\n                items.extend(data[\"data\"])\n                if not data[\"meta\"].get(\"hasNextPage\", False):\n                    break\n                page += 1\n            elif response.status_code == 429:  # Rate limit exceeded\n                retry_after = response.headers.get(\n                    \"Retry-After\", None\n                )  # Obtain reset time\n                wait_time = (\n                    int(retry_after)\n                    if retry_after and retry_after.isdigit()\n                    else 3600\n                )  # Default to 1 hour if no wait time provided\n                print(\n                    f\"\"\"Rate limit exceeded.\n                    Waiting to {wait_time} seconds to retry.\"\"\"\n                )\n                time.sleep(wait_time)\n                continue\n            else:\n                print(f\"Error fetching page {page}: {response.status_code}\")\n                break\n        return items\n\n    def get_all_dockets_by_agency(self):\n        \"\"\"\n        Retrieves all docket IDs by looping through predefined agencies and\n        stores them in a CSV file.\n        \"\"\"\n        all_dockets = []\n\n        for agency in self.agencies:\n            filter_params = {\"filter[agencyId]\": agency}\n\n            agency_dockets = self.fetch_all_pages(\"dockets\", filter_params)\n\n            for docket in agency_dockets:\n                # Extract relevant information from each docket\n                docket_details = {\n                    \"docket_id\": docket.get(\"id\"),\n                    \"docket_type\": docket.get(\"attributes\", {}).get(\n                        \"docketType\"\n                    ),\n                    \"last_modified\": docket.get(\"attributes\", {}).get(\n                        \"lastModifiedDate\"\n                    ),\n                    \"agency_id\": docket.get(\"attributes\", {}).get(\"agencyId\"),\n                    \"title\": docket.get(\"attributes\", {}).get(\"title\"),\n                    \"obj_id\": docket.get(\"attributes\", {}).get(\"objectId\"),\n                }\n                all_dockets.append(docket_details)\n\n        # Store as pandas dataframe.\n        # We can change this mode of storage if you have a different idea,\n        # I think this is a sensible approach, to limit use of our API calls.\n        df = pd.DataFrame(all_dockets)\n        df.drop_duplicates(\n            subset=[\"docket_id\"], inplace=True\n        )  # Ensure there are no duplicate dockets based on docket_id\n        df.to_csv(\"dockets_detailed.csv\", index=False)\n\n    def fetch_documents_by_date_ranges(self, start_date, end_date):\n        \"\"\"\n        Fetches documents posted within specified date ranges and saves them\n        to a CSV file.\n\n        Args:\n            start_date (datetime.date): Start date for fetching documents.\n            end_date (datetime.date): End date for fetching documents.\n        \"\"\"\n        all_documents = []\n        for start, end in self.generate_date_ranges(start_date, end_date):\n            print(f\"Fetching documents from {start} to {end}\")\n            params = {\n                \"filter[postedDate][ge]\": start.strftime(\"%Y-%m-%d\"),\n                \"filter[postedDate][le]\": end.strftime(\"%Y-%m-%d\"),\n            }\n            documents = self.fetch_all_pages(\"documents\", params)\n            all_documents.extend(documents)\n\n        print(f\"Total documents fetched: {len(all_documents)}\")\n\n        # Extract relevant data from documents\n        document_lst = []\n        for document in all_documents:\n            doc_data = {\n                \"Doc_ID\": document.get(\"id\"),\n                \"Doc_Type\": document.get(\"attributes\", {}).get(\"documentType\"),\n                \"Last_Modified\": document.get(\"attributes\", {}).get(\n                    \"lastModifiedDate\"\n                ),\n                \"FR_Doc_Num\": document.get(\"attributes\", {}).get(\"frDocNum\"),\n                \"Withdrawn\": document.get(\"attributes\", {}).get(\"withdrawn\"),\n                \"Agency_ID\": document.get(\"attributes\", {}).get(\"agencyId\"),\n                \"Comment_End_Date\": document.get(\"attributes\", {}).get(\n                    \"commentEndDate\"\n                ),\n                \"Title\": document.get(\"attributes\", {}).get(\"title\"),\n                \"Posted_Date\": document.get(\"attributes\", {}).get(\"postedDate\"),\n                \"Docket_ID\": document.get(\"attributes\", {}).get(\"docketId\"),\n                \"Subtype\": document.get(\"attributes\", {}).get(\"subtype\"),\n                \"Comment_Start_Date\": document.get(\"attributes\", {}).get(\n                    \"commentStartDate\"\n                ),\n                \"Open_For_Comment\": document.get(\"attributes\", {}).get(\n                    \"openForComment\"\n                ),\n                \"Object_ID\": document.get(\"attributes\", {}).get(\"objectId\"),\n            }\n            document_lst.append(doc_data)\n\n        # Save to DataFrame and CSV\n        df = pd.DataFrame(document_lst)\n        df = df.drop_duplicates()\n        df.to_csv(\"doc_detailed_2024.csv\", index=False)\n\n    @staticmethod  # for now, we can put this in utils if that is preferred.\n    def generate_date_ranges(start_date, end_date):\n        \"\"\"\n        Generates weekly date ranges between two dates, inclusive.\n        Helped function for fetch_documents_by_date_ranges().\n\n        Args:\n            start_date (datetime.date): The start date of the range.\n            end_date (datetime.date): The end date of the range.\n\n        Yields:\n            tuple: A tuple of (start_date, end_date) for each week within the\n                specified range.\n        \"\"\"\n        current_date = start_date\n        while current_date &lt; end_date:\n            week_end = current_date + datetime.timedelta(days=6)\n            yield (current_date, min(week_end, end_date))\n            current_date = week_end + datetime.timedelta(days=1)\n\n    def fetch_comment_count_by_documents(self, document_ids, file_output_path):\n        \"\"\"\n        Fetches comments count for each document ID that is open for comments.\n\n        Args:\n            document_ids (DataFrame): DataFrame containing document IDs under\n            the column 'Object_ID'.\n            This can be obtained from the output of\n            fetch_documents_by_date_ranges()\n            file_output_path (str): Path to save the output csv file.\n\n        Returns:\n            None: Results are saved directly to a csv file specified by\n                file_output_path.\n        \"\"\"\n        base_url = f\"{self.base_url}/comments\"\n        results = []\n\n        for commentId in document_ids[\"Object_ID\"]:\n            continue_fetching = True\n            while continue_fetching:\n                params = {\"filter[commentOnId]\": commentId}\n\n                response = requests.get(\n                    base_url, headers=self.headers, params=params\n                )\n                if response.status_code == 200:\n                    data = response.json()\n                    total_elements = data[\"meta\"][\"totalElements\"]\n                    results.append(\n                        {\"id\": commentId, \"total_elements\": total_elements}\n                    )\n                    continue_fetching = False\n                elif response.status_code == 429:  # Rate limit exceeded\n                    retry_after = response.headers.get(\"Retry-After\", None)\n                    wait_time = (\n                        int(retry_after)\n                        if retry_after and retry_after.isdigit()\n                        else 3600\n                    )\n                    print(\n                        f\"\"\"Rate limit exceeded.\n                        Waiting {wait_time} seconds to retry.\"\"\"\n                    )\n                    time.sleep(wait_time)\n                else:\n                    results.append(\n                        {\"id\": commentId, \"total_elements\": \"Failed to fetch\"}\n                    )\n                    continue_fetching = False\n\n        results_df = pd.DataFrame(results)\n        results_df.to_csv(file_output_path)\n</code></pre>"},{"location":"collect/#civiclens.collect.bulk_dl.BulkDl.__init__","title":"<code>__init__(api_key)</code>","text":"<p>Initializes the BulkDl class.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API key for authenticating requests to the</p> required <p>Attributes:</p> Name Type Description <code>api_key</code> <code>str</code> <p>Stored API key for requests.</p> <code>base_url</code> <code>str</code> <p>Base URL for the API endpoint.</p> <code>headers</code> <code>dict</code> <p>Headers to include in API requests, containing API key and content type.</p> <code>agencies</code> <code>list[str]</code> <p>List of agency identifiers (aggregated from https://www.regulations.gov/agencies) to be used in API calls.</p> Source code in <code>civiclens/collect/bulk_dl.py</code> <pre><code>def __init__(self, api_key):\n    \"\"\"\n    Initializes the BulkDl class.\n\n    Args:\n        api_key (str): API key for authenticating requests to the\n        regulations.gov API.\n\n    Attributes:\n        api_key (str): Stored API key for requests.\n        base_url (str): Base URL for the API endpoint.\n        headers (dict): Headers to include in API requests, containing API\n            key and content type.\n        agencies (list[str]): List of agency identifiers (aggregated from\n            https://www.regulations.gov/agencies) to be used in API calls.\n    \"\"\"\n    self.api_key = api_key\n    self.base_url = \"https://api.regulations.gov/v4\"\n    self.headers = {\n        \"X-Api-Key\": self.api_key,\n        \"Content-Type\": \"application/json\",\n    }\n    self.agencies = [\n        \"AID\",\n        \"ATBCB\",\n        \"CFPB\",\n        \"CNCS\",\n        \"COLC\",\n        \"CPPBSD\",\n        \"CPSC\",\n        \"CSB\",\n        \"DHS\",\n        \"DOC\",\n        \"DOD\",\n        \"DOE\",\n        \"DOI\",\n        \"DOJ\",\n        \"DOL\",\n        \"DOS\",\n        \"DOT\",\n        \"ED\",\n        \"EEOC\",\n        \"EIB\",\n        \"EOP\",\n        \"EPA\",\n        \"FFIEC\",\n        \"FMC\",\n        \"FRTIB\",\n        \"FTC\",\n        \"GAPFAC\",\n        \"GSA\",\n        \"HHS\",\n        \"HUD\",\n        \"NARA\",\n        \"NASA\",\n        \"NCUA\",\n        \"NLRB\",\n        \"NRC\",\n        \"NSF\",\n        \"NTSB\",\n        \"ONCD\",\n        \"OPM\",\n        \"PBGC\",\n        \"PCLOB\",\n        \"SBA\",\n        \"SSA\",\n        \"TREAS\",\n        \"USC\",\n        \"USDA\",\n        \"VA\",\n        \"ACF\",\n        \"ACL\",\n        \"AHRQ\",\n        \"AID\",\n        \"AMS\",\n        \"AOA\",\n        \"APHIS\",\n        \"ARS\",\n        \"ASC\",\n        \"ATBCB\",\n        \"ATF\",\n        \"ATR\",\n        \"ATSDR\",\n        \"BIA\",\n        \"BIS\",\n        \"BLM\",\n        \"BLS\",\n        \"BOEM\",\n        \"BOP\",\n        \"BOR\",\n        \"BPA\",\n        \"BPD\",\n        \"BSC\",\n        \"BSEE\",\n        \"CCC\",\n        \"CDC\",\n        \"CDFI\",\n        \"CEQ\",\n        \"CFPB\",\n        \"CISA\",\n        \"CMS\",\n        \"CNCS\",\n        \"COE\",\n        \"COLC\",\n        \"CPPBSD\",\n        \"CPSC\",\n        \"CSB\",\n        \"CSEO\",\n        \"CSREES\",\n        \"DARS\",\n        \"DEA\",\n        \"DEPO\",\n        \"DHS\",\n        \"DOC\",\n        \"DOD\",\n        \"DOE\",\n        \"DOI\",\n        \"DOJ\",\n        \"DOL\",\n        \"DOS\",\n        \"DOT\",\n        \"EAB\",\n        \"EAC\",\n        \"EBSA\",\n        \"ECAB\",\n        \"ECSA\",\n        \"ED\",\n        \"EDA\",\n        \"EEOC\",\n        \"EERE\",\n        \"EIA\",\n        \"EIB\",\n        \"EOA\",\n        \"EOIR\",\n        \"EPA\",\n        \"ERS\",\n        \"ESA\",\n        \"ETA\",\n        \"FAA\",\n        \"FAR\",\n        \"FAS\",\n        \"FBI\",\n        \"FCIC\",\n        \"FCSC\",\n        \"FDA\",\n        \"FEMA\",\n        \"FFIEC\",\n        \"FHWA\",\n        \"FINCEN\",\n        \"FIRSTNET\",\n        \"FISCAL\",\n        \"FLETC\",\n        \"FMC\",\n        \"FMCSA\",\n        \"FNS\",\n        \"FPAC\",\n        \"FRA\",\n        \"FRTIB\",\n        \"FS\",\n        \"FSA\",\n        \"FSIS\",\n        \"FSOC\",\n        \"FTA\",\n        \"FTC\",\n        \"FTZB\",\n        \"FWS\",\n        \"GAPFAC\",\n        \"GIPSA\",\n        \"GSA\",\n        \"HHS\",\n        \"HHSIG\",\n        \"HRSA\",\n        \"HUD\",\n        \"IACB\",\n        \"ICEB\",\n        \"IHS\",\n        \"IPEC\",\n        \"IRS\",\n        \"ISOO\",\n        \"ITA\",\n        \"LMSO\",\n        \"MARAD\",\n        \"MBDA\",\n        \"MMS\",\n        \"MSHA\",\n        \"NAL\",\n        \"NARA\",\n        \"NASA\",\n        \"NASS\",\n        \"NCS\",\n        \"NCUA\",\n        \"NEO\",\n        \"NHTSA\",\n        \"NIC\",\n        \"NIFA\",\n        \"NIGC\",\n        \"NIH\",\n        \"NIST\",\n        \"NLRB\",\n        \"NNSA\",\n        \"NOAA\",\n        \"NPS\",\n        \"NRC\",\n        \"NRCS\",\n        \"NSF\",\n        \"NSPC\",\n        \"NTIA\",\n        \"NTSB\",\n        \"OCC\",\n        \"OEPNU\",\n        \"OFAC\",\n        \"OFCCP\",\n        \"OFPP\",\n        \"OFR\",\n        \"OJJDP\",\n        \"OJP\",\n        \"OMB\",\n        \"ONCD\",\n        \"ONDCP\",\n        \"ONRR\",\n        \"OPM\",\n        \"OPPM\",\n        \"OSHA\",\n        \"OSM\",\n        \"OSTP\",\n        \"OTS\",\n        \"PBGC\",\n        \"PCLOB\",\n        \"PHMSA\",\n        \"PTO\",\n        \"RBS\",\n        \"RHS\",\n        \"RITA\",\n        \"RMA\",\n        \"RTB\",\n        \"RUS\",\n        \"SAMHSA\",\n        \"SBA\",\n        \"SEPA\",\n        \"SLSDC\",\n        \"SSA\",\n        \"SWPA\",\n        \"TA\",\n        \"TREAS\",\n        \"TSA\",\n        \"TTB\",\n        \"USA\",\n        \"USAF\",\n        \"USBC\",\n        \"USCBP\",\n        \"USCG\",\n        \"USCIS\",\n        \"USDA\",\n        \"USDAIG\",\n        \"USGS\",\n        \"USMINT\",\n        \"USN\",\n        \"USPC\",\n        \"USTR\",\n        \"VA\",\n        \"VETS\",\n        \"WAPA\",\n        \"WCPO\",\n        \"WHD\",\n    ]\n</code></pre>"},{"location":"collect/#civiclens.collect.bulk_dl.BulkDl.fetch_all_pages","title":"<code>fetch_all_pages(endpoint, params, max_items_per_page=250)</code>","text":"<p>Iterates through all the pages of API data for a given endpoint until there are no more pages to fetch (occurs at 20 pages, or 5000 items).</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The API endpoint to fetch data from.             ['dockets', 'documents', 'comments']</p> required <code>params</code> <code>dict</code> <p>Dictionary of parameters to send in the API request.</p> required <code>max_items_per_page</code> <code>int</code> <p>Maximum number of items per page to</p> <code>250</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of items (dictionaries) fetched from all pages of the API endpoint.</p> Source code in <code>civiclens/collect/bulk_dl.py</code> <pre><code>def fetch_all_pages(self, endpoint, params, max_items_per_page=250):\n    \"\"\"\n    Iterates through all the pages of API data for a given endpoint\n    until there are no more pages to fetch (occurs at 20 pages, or\n    5000 items).\n\n    Args:\n        endpoint (str): The API endpoint to fetch data from.\n                        ['dockets', 'documents', 'comments']\n        params (dict): Dictionary of parameters to send in the API request.\n        max_items_per_page (int): Maximum number of items per page to\n        request. Max (default) = 250.\n\n    Returns:\n        list: A list of items (dictionaries) fetched from all pages of the\n            API endpoint.\n    \"\"\"\n    items = []\n    page = 1\n    while True:\n        params[\"page[number]\"] = page\n        params[\"page[size]\"] = max_items_per_page\n\n        response = requests.get(\n            f\"{self.base_url}/{endpoint}\",\n            headers=self.headers,\n            params=params,\n        )\n        print(f\"Requesting: {response.url}\")\n\n        if response.status_code == 200:\n            try:\n                data = response.json()\n            except ValueError:\n                print(\"Failed to decode JSON response\")\n                break\n            items.extend(data[\"data\"])\n            if not data[\"meta\"].get(\"hasNextPage\", False):\n                break\n            page += 1\n        elif response.status_code == 429:  # Rate limit exceeded\n            retry_after = response.headers.get(\n                \"Retry-After\", None\n            )  # Obtain reset time\n            wait_time = (\n                int(retry_after)\n                if retry_after and retry_after.isdigit()\n                else 3600\n            )  # Default to 1 hour if no wait time provided\n            print(\n                f\"\"\"Rate limit exceeded.\n                Waiting to {wait_time} seconds to retry.\"\"\"\n            )\n            time.sleep(wait_time)\n            continue\n        else:\n            print(f\"Error fetching page {page}: {response.status_code}\")\n            break\n    return items\n</code></pre>"},{"location":"collect/#civiclens.collect.bulk_dl.BulkDl.fetch_comment_count_by_documents","title":"<code>fetch_comment_count_by_documents(document_ids, file_output_path)</code>","text":"<p>Fetches comments count for each document ID that is open for comments.</p> <p>Parameters:</p> Name Type Description Default <code>document_ids</code> <code>DataFrame</code> <p>DataFrame containing document IDs under</p> required <code>file_output_path</code> <code>str</code> <p>Path to save the output csv file.</p> required <p>Returns:</p> Name Type Description <code>None</code> <p>Results are saved directly to a csv file specified by file_output_path.</p> Source code in <code>civiclens/collect/bulk_dl.py</code> <pre><code>def fetch_comment_count_by_documents(self, document_ids, file_output_path):\n    \"\"\"\n    Fetches comments count for each document ID that is open for comments.\n\n    Args:\n        document_ids (DataFrame): DataFrame containing document IDs under\n        the column 'Object_ID'.\n        This can be obtained from the output of\n        fetch_documents_by_date_ranges()\n        file_output_path (str): Path to save the output csv file.\n\n    Returns:\n        None: Results are saved directly to a csv file specified by\n            file_output_path.\n    \"\"\"\n    base_url = f\"{self.base_url}/comments\"\n    results = []\n\n    for commentId in document_ids[\"Object_ID\"]:\n        continue_fetching = True\n        while continue_fetching:\n            params = {\"filter[commentOnId]\": commentId}\n\n            response = requests.get(\n                base_url, headers=self.headers, params=params\n            )\n            if response.status_code == 200:\n                data = response.json()\n                total_elements = data[\"meta\"][\"totalElements\"]\n                results.append(\n                    {\"id\": commentId, \"total_elements\": total_elements}\n                )\n                continue_fetching = False\n            elif response.status_code == 429:  # Rate limit exceeded\n                retry_after = response.headers.get(\"Retry-After\", None)\n                wait_time = (\n                    int(retry_after)\n                    if retry_after and retry_after.isdigit()\n                    else 3600\n                )\n                print(\n                    f\"\"\"Rate limit exceeded.\n                    Waiting {wait_time} seconds to retry.\"\"\"\n                )\n                time.sleep(wait_time)\n            else:\n                results.append(\n                    {\"id\": commentId, \"total_elements\": \"Failed to fetch\"}\n                )\n                continue_fetching = False\n\n    results_df = pd.DataFrame(results)\n    results_df.to_csv(file_output_path)\n</code></pre>"},{"location":"collect/#civiclens.collect.bulk_dl.BulkDl.fetch_documents_by_date_ranges","title":"<code>fetch_documents_by_date_ranges(start_date, end_date)</code>","text":"<p>Fetches documents posted within specified date ranges and saves them to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>date</code> <p>Start date for fetching documents.</p> required <code>end_date</code> <code>date</code> <p>End date for fetching documents.</p> required Source code in <code>civiclens/collect/bulk_dl.py</code> <pre><code>def fetch_documents_by_date_ranges(self, start_date, end_date):\n    \"\"\"\n    Fetches documents posted within specified date ranges and saves them\n    to a CSV file.\n\n    Args:\n        start_date (datetime.date): Start date for fetching documents.\n        end_date (datetime.date): End date for fetching documents.\n    \"\"\"\n    all_documents = []\n    for start, end in self.generate_date_ranges(start_date, end_date):\n        print(f\"Fetching documents from {start} to {end}\")\n        params = {\n            \"filter[postedDate][ge]\": start.strftime(\"%Y-%m-%d\"),\n            \"filter[postedDate][le]\": end.strftime(\"%Y-%m-%d\"),\n        }\n        documents = self.fetch_all_pages(\"documents\", params)\n        all_documents.extend(documents)\n\n    print(f\"Total documents fetched: {len(all_documents)}\")\n\n    # Extract relevant data from documents\n    document_lst = []\n    for document in all_documents:\n        doc_data = {\n            \"Doc_ID\": document.get(\"id\"),\n            \"Doc_Type\": document.get(\"attributes\", {}).get(\"documentType\"),\n            \"Last_Modified\": document.get(\"attributes\", {}).get(\n                \"lastModifiedDate\"\n            ),\n            \"FR_Doc_Num\": document.get(\"attributes\", {}).get(\"frDocNum\"),\n            \"Withdrawn\": document.get(\"attributes\", {}).get(\"withdrawn\"),\n            \"Agency_ID\": document.get(\"attributes\", {}).get(\"agencyId\"),\n            \"Comment_End_Date\": document.get(\"attributes\", {}).get(\n                \"commentEndDate\"\n            ),\n            \"Title\": document.get(\"attributes\", {}).get(\"title\"),\n            \"Posted_Date\": document.get(\"attributes\", {}).get(\"postedDate\"),\n            \"Docket_ID\": document.get(\"attributes\", {}).get(\"docketId\"),\n            \"Subtype\": document.get(\"attributes\", {}).get(\"subtype\"),\n            \"Comment_Start_Date\": document.get(\"attributes\", {}).get(\n                \"commentStartDate\"\n            ),\n            \"Open_For_Comment\": document.get(\"attributes\", {}).get(\n                \"openForComment\"\n            ),\n            \"Object_ID\": document.get(\"attributes\", {}).get(\"objectId\"),\n        }\n        document_lst.append(doc_data)\n\n    # Save to DataFrame and CSV\n    df = pd.DataFrame(document_lst)\n    df = df.drop_duplicates()\n    df.to_csv(\"doc_detailed_2024.csv\", index=False)\n</code></pre>"},{"location":"collect/#civiclens.collect.bulk_dl.BulkDl.generate_date_ranges","title":"<code>generate_date_ranges(start_date, end_date)</code>  <code>staticmethod</code>","text":"<p>Generates weekly date ranges between two dates, inclusive. Helped function for fetch_documents_by_date_ranges().</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>date</code> <p>The start date of the range.</p> required <code>end_date</code> <code>date</code> <p>The end date of the range.</p> required <p>Yields:</p> Name Type Description <code>tuple</code> <p>A tuple of (start_date, end_date) for each week within the specified range.</p> Source code in <code>civiclens/collect/bulk_dl.py</code> <pre><code>@staticmethod  # for now, we can put this in utils if that is preferred.\ndef generate_date_ranges(start_date, end_date):\n    \"\"\"\n    Generates weekly date ranges between two dates, inclusive.\n    Helped function for fetch_documents_by_date_ranges().\n\n    Args:\n        start_date (datetime.date): The start date of the range.\n        end_date (datetime.date): The end date of the range.\n\n    Yields:\n        tuple: A tuple of (start_date, end_date) for each week within the\n            specified range.\n    \"\"\"\n    current_date = start_date\n    while current_date &lt; end_date:\n        week_end = current_date + datetime.timedelta(days=6)\n        yield (current_date, min(week_end, end_date))\n        current_date = week_end + datetime.timedelta(days=1)\n</code></pre>"},{"location":"collect/#civiclens.collect.bulk_dl.BulkDl.get_all_dockets_by_agency","title":"<code>get_all_dockets_by_agency()</code>","text":"<p>Retrieves all docket IDs by looping through predefined agencies and stores them in a CSV file.</p> Source code in <code>civiclens/collect/bulk_dl.py</code> <pre><code>def get_all_dockets_by_agency(self):\n    \"\"\"\n    Retrieves all docket IDs by looping through predefined agencies and\n    stores them in a CSV file.\n    \"\"\"\n    all_dockets = []\n\n    for agency in self.agencies:\n        filter_params = {\"filter[agencyId]\": agency}\n\n        agency_dockets = self.fetch_all_pages(\"dockets\", filter_params)\n\n        for docket in agency_dockets:\n            # Extract relevant information from each docket\n            docket_details = {\n                \"docket_id\": docket.get(\"id\"),\n                \"docket_type\": docket.get(\"attributes\", {}).get(\n                    \"docketType\"\n                ),\n                \"last_modified\": docket.get(\"attributes\", {}).get(\n                    \"lastModifiedDate\"\n                ),\n                \"agency_id\": docket.get(\"attributes\", {}).get(\"agencyId\"),\n                \"title\": docket.get(\"attributes\", {}).get(\"title\"),\n                \"obj_id\": docket.get(\"attributes\", {}).get(\"objectId\"),\n            }\n            all_dockets.append(docket_details)\n\n    # Store as pandas dataframe.\n    # We can change this mode of storage if you have a different idea,\n    # I think this is a sensible approach, to limit use of our API calls.\n    df = pd.DataFrame(all_dockets)\n    df.drop_duplicates(\n        subset=[\"docket_id\"], inplace=True\n    )  # Ensure there are no duplicate dockets based on docket_id\n    df.to_csv(\"dockets_detailed.csv\", index=False)\n</code></pre>"},{"location":"models/","title":"Models","text":""},{"location":"models/#modelresource-documentation","title":"Model/Resource Documentation","text":"<p>The following describes the tables/models comprising our relational database. Each model features a short description noting its purpose and key fields. A table listing field names, datatypes, and a short description of each field is also included. The <code>Dockets</code>, <code>PublicComments</code>, and <code>Documents</code> tables mimic the regulations.gov tables, while the <code>NLP Output</code> table is our data generated based on the regulations.gov data tables that is used to populate the website. Where adequate NLP output can't be generated due to data sparsity, information from <code>PublicComments</code> and <code>Documents</code> is used.</p> <p>Dockets</p> <p><code>Dockets</code> represents collections of documents relevant to a proposed rule or notice. A given docket can contain documents available for commenting that represent the proposed rule change along with supplementary documents, such as a cost-benefit analysis, that support the proposed rule.</p> <p>Since the dockets themselves are unavailable to comment on (only the documents contained within can recieve comments), we chose to collect only enough information to link docments to a docket (via the <code>id</code> field ) and other basic information (date, posting agency).</p> Name Type Description id PrimaryKey (Dockets) Regulations.gov UUID for a docket docketType CharField Type of docket (i.e. Rulemaking, Nonrulemaking) lastModifiedDate DateTime Date docket was last updated agencyID CharField ID of agency who posted docket objectID CharField Regulations.gov UUID for API response object <p>PublicComments</p> <p><code>PublicComments</code> represents an individual comment posted to a document. Each comment features its own UUID (<code>id</code>) and is linked to its corresponding document by the <code>document</code> id (also links this table to the <code>Documents</code> table). Each comment is represented by the text of the comment, along with any available information collected by Regulations.gov on the comment. This data describes both the comment, such as whether the comment was withdrawn by the user, if the commented was posted to a restricted document (only open to certain agencies or interest groups), or the number of comments that feature the same text.</p> <p>This table also stores data on individuals who posted a comment, including their name, location, and organization.</p> Name Type Description id PrimaryKey (Comment) Regulations.gov UUID for a document commentOn CharField Document the commnent is posted to document ForeignKey (Documents) Document ID a comment is posted to duplicateComments IntegerField Number of duplicate comments stateProvinceRegion CharField State or province a comment is posted from subtype CharField Classifier for source of comment (e.g Member of Congress, Mass Mail Campaign) objectId CharField Regulations.gov UUID for API response object comment TextField Text of comment firstName CharField First name of commenter lastName CharField Last name of commenter address1 CharField First line of commenter's address address2 CharField Second line of commenter's address city CharField City of the commenter's address country CharField Commenter's country email EmailField Email address of commenter phone CharField Phone number of commenter govAgency CharField Agency receiving comments govAgencyType CharField Type of agency receiving comments organization CharField Commenter's organization originalDocumentId CharField Regulations.gov document ID modifyDate DateTime Date the comment was last modified pageCount IntegerField Number of pages for the comment postedDate DateTime Date the comment was inital posted receiveDate DateTime Date comment was recieved by posting agency title CharField Title of the commenter withdrawn Boolean Bool if the comment was withdrawn reasonWithdrawn CharField User submitted reason a comment was withdrawn zip charField Zip code of the commenter restrictReasonType CharField If document has been restricted for comment to certain users restrictReason CharField Summary of reason for comment restriction <p>Documents</p> <p><code>Documents</code> stores metadata on documents that can have comments posted to. To minimize our data intake, CivicLens only collects documents labeled as open-for-comment by the federal government. This does not mean that every document we collect features posted comments, but that some subset of the public is able to comment on each document. Each document also stores a URL to the Federal Register's XML version of the full document text (<code>fullTextXmlUrl</code>). This is used to extract the full text of the proposed rule which is not available through regulations.gov. <code>summary</code> contains the plain English summary of the rule written by the Federal government which will be available on our site under the <code>/documents</code> endpoint.</p> Name Type Description id PrimaryKey (Documents) Regulations.gov UUID for a document documentType CharField Type of document (i.e. Proposed Rule, Notice) lastModifiedDate DateTime Date document was last updated withdrawn BooleanField Boolean if the documentment was withdrawn agencyID CharField ID of agency who posted document commentEndDate DateTime End date of document commenting period commentStartDate DateTime Start date of document commenting period objectId CharField Regulations.gov UUID for API response object fullTextXmlUrl URL Link to Federal Registar's XML text of rule subAgy CharField Relevant office or department of posting agency agencyType CharField Name of posting agency (e.g. FDA) CRF CharField Code of Federal Regulations number RIN CharField Regulation Identification Number used by Federal Register title CharField Regulations.gov document title summary TextField Plain English summary of document furtherInformation TextField Additional information provided with document <p>NLP Output</p> <p>The <code>NLP Output</code> table stores the information generated by the NLP pipeline that is run by our website. There is one row for each document in the database which contains information on representative comments, new titles, sentiment, and more. This data is used to populate the <code>search</code> and <code>document</code> pages on the website with relevant information about a document.</p> Name Type Description document_id OneToOneField (Documents) Regulations.gov UUID for a document comments JSONField Representative comments that can be form letters or unique comments doc_plain_english_title CharField AI generated simple title for a document num_total_comments IntegerField Number of all comments on a document num_unique_comments IntegerField Number of unique submissions including types of form letters num_representative_comment IntegerField Number of representative comments topics JSONField NLP generated topics for the document from the representative comments num_topics IntegerField Number of topics identified last_updated DateTimeField Last time the NLP was updated for the document created_at DateTimeField Time the document was created search_topics TextField NLP generated topics for the django search to reference is_representative BooleanField Does the document have representative comments"},{"location":"nlp/","title":"nlp","text":"<p>API reference for CivicLen's natural language processing toolkit.</p>"},{"location":"nlp/#reference","title":"Reference","text":""},{"location":"nlp/#civiclens.nlp.comments","title":"<code>comments</code>","text":""},{"location":"nlp/#civiclens.nlp.comments.assign_clusters","title":"<code>assign_clusters(df, clusters)</code>","text":"<p>Inserts cluster info into the polars df of data from the initial pull</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df from initial pull</p> required <code>clusters</code> <code>list[set[int]]</code> <p>clusters from Louvain Communities</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: updated df</p> Source code in <code>civiclens/nlp/comments.py</code> <pre><code>def assign_clusters(df: pl.DataFrame, clusters: list[set[int]]) -&gt; pl.DataFrame:\n    \"\"\"Inserts cluster info into the polars df of data from the initial pull\n\n    Args:\n        df (pl.DataFrame): df from initial pull\n        clusters (list[set[int]]): clusters from Louvain Communities\n\n    Returns:\n        pl.DataFrame: updated df\n    \"\"\"\n    rows = df.shape[0]\n    # go through clusters and add that info to df\n    for i, cluster_data in enumerate(clusters):\n        df = df.with_columns(\n            pl.when(pl.arange(0, rows).is_in(cluster_data))\n            .then(i)\n            .otherwise(pl.col(\"cluster\"))\n            .alias(\"cluster\")\n        )\n\n    return df\n</code></pre>"},{"location":"nlp/#civiclens.nlp.comments.build_graph","title":"<code>build_graph(df)</code>","text":"<p>Builds a network graph with comments as nodes and their similarities as weights</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with pairs of comment indices and a cosine similarity</p> required <p>Returns:</p> Type Description <code>Graph</code> <p>nx.Graph:network graph with comments as nodes and their similarities as weights</p> Source code in <code>civiclens/nlp/comments.py</code> <pre><code>def build_graph(df: pl.DataFrame) -&gt; nx.Graph:\n    \"\"\"Builds a network graph with comments as nodes and their similarities as\n    weights\n\n    Args:\n        df (pl.DataFrame): df with pairs of comment indices and a cosine\n            similarity\n\n    Returns:\n        nx.Graph:network graph with comments as nodes and their similarities as\n            weights\n    \"\"\"\n    graph_data = df.to_dicts()\n    G = nx.Graph()\n    for edge in graph_data:\n        G.add_edge(edge[\"idx1\"], edge[\"idx2\"], weight=edge[\"similarity\"])\n\n    return G\n</code></pre>"},{"location":"nlp/#civiclens.nlp.comments.comment_similarity","title":"<code>comment_similarity(df, model)</code>","text":"<p>Create df with comment mappings and their semantic similarity scores according to the SBERT paraphrase mining method using the all-mpnet-base-v2 model from hugging face.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>df with comment data</p> required <code>model</code> <code>SentenceTransformer</code> <p>sbert sentence transformer model</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>df_paraphrase, df_form_letter (tuple[pl.DataFrame]): cosine similarities for form letters and non form letters</p> Source code in <code>civiclens/nlp/comments.py</code> <pre><code>def comment_similarity(\n    df: pl.DataFrame, model: SentenceTransformer\n) -&gt; pl.DataFrame:\n    \"\"\"Create df with comment mappings and their semantic similarity scores\n    according to the SBERT paraphrase mining method using the all-mpnet-base-v2\n    model from hugging face.\n\n    Args:\n        df (pl.DataFrame): df with comment data\n        model (SentenceTransformer): sbert sentence transformer model\n\n    Returns:\n        df_paraphrase, df_form_letter (tuple[pl.DataFrame]): cosine\n            similarities for form letters and non form letters\n    \"\"\"\n    paraphrases = util.paraphrase_mining(\n        model, df[\"comment\"].to_list(), show_progress_bar=True\n    )\n    df_full = pl.DataFrame(\n        {\n            \"similarity\": pl.Series(\n                \"similarity\", [x[0] for x in paraphrases], dtype=pl.Float64\n            ),\n            \"idx1\": pl.Series(\n                \"idx1\", [x[1] for x in paraphrases], dtype=pl.Int64\n            ),\n            \"idx2\": pl.Series(\n                \"idx2\", [x[2] for x in paraphrases], dtype=pl.Int64\n            ),\n        }\n    )\n\n    df_paraphrases = df_full.filter(pl.col(\"similarity\") &lt;= 0.99)\n    df_paraphrases = df_paraphrases.with_columns(\n        pl.lit(False).alias(\"form_letter\")\n    )\n\n    df_form_letter = df_full.filter(pl.col(\"similarity\") &gt; 0.99)\n    df_form_letter = df_form_letter.with_columns(\n        pl.lit(True).alias(\"form_letter\")\n    )\n\n    return df_paraphrases, df_form_letter\n</code></pre>"},{"location":"nlp/#civiclens.nlp.comments.compute_similiarity_clusters","title":"<code>compute_similiarity_clusters(embeds, sim_threshold)</code>","text":"<p>Extract form letters from corpus of comments.</p> <p>Parameters:</p> Name Type Description Default <code>embeds</code> <code>ndarray</code> <p>array of embeddings representing the documents</p> required <code>sim_threshold</code> <code>float</code> <p>distance thresholds to divide clusters</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of docs by cluster</p> Source code in <code>civiclens/nlp/comments.py</code> <pre><code>def compute_similiarity_clusters(\n    embeds: np.ndarray, sim_threshold: float\n) -&gt; np.ndarray:\n    \"\"\"\n    Extract form letters from corpus of comments.\n\n    Args:\n        embeds: array of embeddings representing the documents\n        sim_threshold: distance thresholds to divide clusters\n\n    Returns:\n        Array of docs by cluster\n    \"\"\"\n    kmeans = AgglomerativeClustering(\n        n_clusters=None,\n        metric=\"cosine\",\n        linkage=\"average\",\n        distance_threshold=sim_threshold,\n    )\n    kmeans.fit(embeds)\n\n    return kmeans.labels_\n</code></pre>"},{"location":"nlp/#civiclens.nlp.comments.count_unique_comments","title":"<code>count_unique_comments(df)</code>","text":"<p>Counts number of unique comments identified by performing paraphrasing mining on a corpus of comments.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe of similiar comments</p> required Source code in <code>civiclens/nlp/comments.py</code> <pre><code>def count_unique_comments(df: pl.DataFrame) -&gt; int:\n    \"\"\"\n    Counts number of unique comments identified by performing paraphrasing\n    mining on a corpus of comments.\n\n    Args:\n        df: dataframe of similiar comments\n    \"\"\"\n    indices = df[\"idx1\"].to_list() + df[\"idx2\"].to_list()\n\n    return len(set(indices))\n</code></pre>"},{"location":"nlp/#civiclens.nlp.comments.find_central_node","title":"<code>find_central_node(G, clusters)</code>","text":"<p>Find the most representative comment in a cluster by identifying the most central node</p> <p>Parameters:</p> Name Type Description Default <code>G</code> <code>Graph</code> <p>network graph with comments as nodes and their similarities as weights</p> required <code>clusters</code> <code>list[set[int]]</code> <p>clusters from Louvain Communities</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the central comment id as the key and the degree centrality as the value</p> Source code in <code>civiclens/nlp/comments.py</code> <pre><code>def find_central_node(G: nx.Graph, clusters: list[set[int]]) -&gt; dict:\n    \"\"\"Find the most representative comment in a cluster by identifying the\n    most central node\n\n    Args:\n        G (nx.Graph): network graph with comments as nodes and their\n            similarities as weights\n        clusters (list[set[int]]): clusters from Louvain Communities\n\n    Returns:\n        dict: dictionary with the central comment id as the key and the degree\n            centrality as the value\n    \"\"\"\n    centrality_per_cluster = {}\n    for cluster in clusters:\n        # focus on each specific cluster of comments\n        subgraph = G.subgraph(cluster)\n        # calculate the centrality of each comment in the cluster\n        centralities = nx.degree_centrality(subgraph)\n        # Find the node with the highest centrality\n        central_node = max(centralities, key=centralities.get)\n        centrality_per_cluster[central_node] = centralities[central_node]\n\n    return centrality_per_cluster\n</code></pre>"},{"location":"nlp/#civiclens.nlp.comments.find_form_letters","title":"<code>find_form_letters(df, model, form_threshold)</code>","text":"<p>Finds and extracts from letters by clustering, counts number of unique comments.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe of comments to extract form letters from</p> required <code>model</code> <code>SentenceTransformer</code> <p>vectorize model for text embeddings</p> required <code>form_threshold</code> <code>int</code> <p>threshold to consider a comment a form letter</p> required <p>Returns:</p> Type Description <code>tuple[list[dict], int]</code> <p>List of form letters, number of unique comments</p> Source code in <code>civiclens/nlp/comments.py</code> <pre><code>def find_form_letters(\n    df: pl.DataFrame, model: SentenceTransformer, form_threshold: int\n) -&gt; tuple[list[dict], int]:\n    \"\"\"\n    Finds and extracts from letters by clustering, counts number of unique\n    comments.\n\n    Args:\n        df: dataframe of comments to extract form letters from\n        model: vectorize model for text embeddings\n        form_threshold: threshold to consider a comment a form letter\n\n    Returns:\n        List of form letters, number of unique comments\n    \"\"\"\n    # TODO clean strings\n    num_form_letters = 0\n    form_letters = []\n    docs = df[\"comment_text\"].to_numpy()\n\n    if len(docs) &lt;= 1:  # cannot cluster with less than 2 documents\n        return form_letters, num_form_letters\n\n    embeds = model.encode(docs, convert_to_numpy=True)\n    clusters = compute_similiarity_clusters(embeds, sim_threshold=0.025)\n    document_id = df.unique(subset=\"document_id\").select(\"document_id\").item()\n\n    num_form_letters += clusters.max() + 1\n    for cluster in range(num_form_letters):\n        cluster_docs = docs[np.where(clusters == cluster)]\n        if cluster_docs.size == 0:\n            continue\n\n        num_rep = (\n            df.filter(pl.col(\"comment_text\").is_in(cluster_docs))\n            .select(\"comments_represented\")\n            .sum()\n            .item()\n        )\n        letter_text = np.random.choice(cluster_docs, size=1).item()\n        letter_id = (\n            df.filter(pl.col(\"comment_text\") == letter_text)\n            .select(\"comment_id\")\n            .item()\n        )\n\n        form_letter = True\n        if num_rep &lt;= form_threshold:\n            form_letter = False\n\n        form_letters.append(\n            {\n                \"comments_represented\": num_rep,\n                \"comment_id\": letter_id,\n                \"document_id\": document_id,\n                \"comment_text\": letter_text,\n                \"form_letter\": form_letter,\n            }\n        )\n\n    return form_letters, num_form_letters\n</code></pre>"},{"location":"nlp/#civiclens.nlp.comments.get_clusters","title":"<code>get_clusters(G)</code>","text":"<p>Defines clusters based on the Louvain Communities algorithm</p> <p>Parameters:</p> Name Type Description Default <code>G</code> <code>Graph</code> <p>network graph with comments as nodes and their similarities as weights</p> required <p>Returns:</p> Type Description <code>list[set[int]]</code> <p>list[set[int]]: sets are clusters of comment nodes</p> Source code in <code>civiclens/nlp/comments.py</code> <pre><code>def get_clusters(G: nx.Graph) -&gt; list[set[int]]:\n    \"\"\"Defines clusters based on the Louvain Communities algorithm\n\n    Args:\n        G (nx.Graph): network graph with comments as nodes and their\n            similarities as weights\n\n    Returns:\n        list[set[int]]: sets are clusters of comment nodes\n    \"\"\"\n    return louvain_communities(G=G)\n</code></pre>"},{"location":"nlp/#civiclens.nlp.comments.get_doc_comments","title":"<code>get_doc_comments(id)</code>","text":"<p>Pulls all comments for a set of documents and preprocesses that into a polars dataframe</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>document id</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: formated polars df</p> Source code in <code>civiclens/nlp/comments.py</code> <pre><code>def get_doc_comments(id: str) -&gt; pl.DataFrame:\n    \"\"\"Pulls all comments for a set of documents and preprocesses that into a\n    polars dataframe\n\n    Args:\n        id (int): document id\n\n    Returns:\n        pl.DataFrame: formated polars df\n    \"\"\"\n    query = f\"\"\"\n        SELECT id, document_id, comment\n        FROM regulations_comment\n        WHERE document_id = '{id}';\n        \"\"\"  # noqa: E702, E231, E241\n    # filter out attached files\n    db = Database()\n    df = pull_data(\n        query=query, connection=db, schema=[\"id\", \"document_id\", \"comment\"]\n    )\n    pattern = (\n        r\"(?i)^see attached file(s)?\\.?$\"\n        r\"|(?i)^please see attached?\\.?$\"\n        r\"|(?i)^see attached?\\.?\"\n        r\"|(?i)^see attached file\\(s\\)\\.?$\"\n    )\n\n    filtered_df = df.filter(~pl.col(\"comment\").str.contains(pattern))\n\n    # TODO create clusters column in comment table and delete these lines\n    rows = filtered_df.shape[0]\n    filtered_df = filtered_df.with_columns(\n        pl.Series(\"cluster\", [None] * rows).cast(pl.Utf8)\n    )\n    return filtered_df\n</code></pre>"},{"location":"nlp/#civiclens.nlp.comments.rep_comment_analysis","title":"<code>rep_comment_analysis(comment_data, df, model)</code>","text":"<p>Runs all representative comment code for a document</p> <p>Parameters:</p> Name Type Description Default <code>comment_data</code> <code>RepComment</code> <p>empty RepComment object</p> required <code>df</code> <code>dataframe</code> <p>dataframe of comments pertaining to a document</p> required <code>model</code> <code>SentenceTransformer</code> <p>SBERT model for embeddings</p> required <p>Returns:</p> Name Type Description <code>RepComment</code> <code>RepComments</code> <p>dataclass with comment data</p> Source code in <code>civiclens/nlp/comments.py</code> <pre><code>def rep_comment_analysis(\n    comment_data: RepComments, df: pl.DataFrame, model: SentenceTransformer\n) -&gt; RepComments:\n    \"\"\"Runs all representative comment code for a document\n\n    Args:\n        comment_data (RepComment): empty RepComment object\n        df (dataframe): dataframe of comments pertaining to a document\n        model (SentenceTransformer): SBERT model for embeddings\n\n    Returns:\n        RepComment: dataclass with comment data\n    \"\"\"\n    df_paraphrases, df_form_letter = comment_similarity(df, model)\n\n    try:\n        G_paraphrase = build_graph(df_paraphrases)\n        clusters_paraphrase = get_clusters(G=G_paraphrase)\n        df = assign_clusters(df=df, clusters=clusters_paraphrase)\n        df_rep_paraphrase = representative_comments(\n            G_paraphrase, clusters_paraphrase, df, form_letter=False\n        ).sort(pl.col(\"comments_represented\"), descending=True)\n    except ZeroDivisionError:\n        print(\"Paraphrase Clustering Not Possible: Empty DataFrame\")\n\n    try:\n        G_form_letter = build_graph(df_form_letter)\n        clusters_form_letter = get_clusters(G=G_form_letter)\n        df = assign_clusters(df=df, clusters=clusters_form_letter)\n        df_rep_form = representative_comments(\n            G_form_letter,\n            clusters_form_letter,\n            df,\n            form_letter=True,\n        ).sort(pl.col(\"comments_represented\"), descending=True)\n    except ZeroDivisionError:\n        print(\"Form Letter Clustering Not Possible: Empty DataFrame\")\n\n    # fill out comment class\n    comment_data.doc_comments = df\n    form_letters, num_form_letters = find_form_letters(\n        df_rep_form, model, form_threshold=10\n    )\n\n    if df_rep_form.is_empty():\n        comment_data.rep_comments = df_rep_paraphrase.to_dicts()\n        comment_data.num_representative_comment = df_rep_paraphrase.shape[0]\n    elif df_rep_paraphrase.is_empty():\n        comment_data.rep_comments = form_letters\n        comment_data.num_representative_comment = num_form_letters\n    else:\n        comment_data.rep_comments = form_letters + df_rep_paraphrase.to_dicts()\n        comment_data.num_representative_comment = len(comment_data.rep_comments)\n\n    num_paraphrased = count_unique_comments(df_paraphrases)\n    comment_data.num_total_comments = df.shape[0]\n    comment_data.num_unique_comments = (\n        num_paraphrased + num_form_letters\n        if num_paraphrased &lt; comment_data.num_total_comments\n        else num_paraphrased\n    )\n\n    return comment_data\n</code></pre>"},{"location":"nlp/#civiclens.nlp.comments.representative_comments","title":"<code>representative_comments(G, clusters, df, form_letter)</code>","text":"<p>Creates a dataframe with the text of the representative comments along with the number of comments that are semantically represented by that text</p> <p>Parameters:</p> Name Type Description Default <code>G</code> <code>Graph</code> <p>network graph with comments as nodes and their similarities as weights</p> required <code>clusters</code> <code>list[set[int]]</code> <p>clusters from Louvain Communities</p> required <code>df</code> <code>DataFrame</code> <p>df from initial pull with added cluster info</p> required <p>Returns:</p> Name Type Description <code>output_df</code> <code>DataFrame</code> <p>df with representation information</p> Source code in <code>civiclens/nlp/comments.py</code> <pre><code>def representative_comments(\n    G: nx.Graph, clusters: list[set[int]], df: pl.DataFrame, form_letter: bool\n) -&gt; pl.DataFrame:\n    \"\"\"Creates a dataframe with the text of the representative comments along\n    with the number of comments that are semantically represented by that text\n\n    Args:\n        G (nx.Graph): network graph with comments as nodes and their\n            similarities as weights\n        clusters (list[set[int]]): clusters from Louvain Communities\n        df (pl.DataFrame): df from initial pull with added cluster info\n\n    Returns:\n        output_df (pl.DataFrame): df with representation information\n    \"\"\"\n    central_nodes = find_central_node(G, clusters)\n    representative_dict = {\n        \"comments_represented\": [],\n        \"comment_id\": [],\n        \"document_id\": [],\n        \"comment_text\": [],\n        \"form_letter\": [],\n    }\n    for i, community in enumerate(clusters):\n        community_size = len(community)\n        central_node = list(central_nodes.keys())[i]\n        representative_dict.get(\"comments_represented\").append(community_size)\n        representative_dict.get(\"comment_id\").append(df[central_node, 0])\n        representative_dict.get(\"document_id\").append(df[central_node, 1])\n        representative_dict.get(\"comment_text\").append(df[central_node, 2])\n        representative_dict.get(\"form_letter\").append(form_letter)\n\n    output_df = pl.DataFrame(representative_dict)\n\n    if form_letter:\n        return output_df.unique(subset=[\"comment_text\"])\n    else:\n        return output_df\n</code></pre>"},{"location":"nlp/#civiclens.nlp.titles","title":"<code>titles</code>","text":""},{"location":"nlp/#civiclens.nlp.titles.TitleChain","title":"<code>TitleChain</code>","text":"<p>Creates more accessible titles for regulation documnents</p> Source code in <code>civiclens/nlp/titles.py</code> <pre><code>class TitleChain:\n    \"\"\"Creates more accessible titles for regulation documnents\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.template = \"\"\"You are a title generator that is given a paragraph\n        summary of a regulation. You job is to create a title that conveys the\n        content of the paragraph summary in a succinct way that highlights the\n        content that would be relevant to someone who is civically engaged and\n        looking to find interesting regulations to comment on.\n\n            Regulation Summary: {paragraph}\n\n            Answer:\"\"\"\n        self.prompt = PromptTemplate.from_template(self.template)\n        self.model = title_model\n        self.tokenizer = title_tokenizer\n        self.pipe = pipeline(\n            \"text2text-generation\",\n            model=self.model,\n            tokenizer=self.tokenizer,\n            max_length=20,\n        )\n        self.hf_pipeline = HuggingFacePipeline(pipeline=self.pipe)\n        self.parse = StrOutputParser()\n        self.chain = self.prompt | self.hf_pipeline | self.parse\n\n    def invoke(self, paragraph: str) -&gt; str:\n        return self.chain.invoke({\"paragraph\": paragraph})\n</code></pre>"},{"location":"nlp/#civiclens.nlp.titles.get_doc_summary","title":"<code>get_doc_summary(id)</code>","text":"<p>Gets the id and summary for a given document</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>document id</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: formatted polars df</p> Source code in <code>civiclens/nlp/titles.py</code> <pre><code>def get_doc_summary(id: str) -&gt; pl.DataFrame:\n    \"\"\"Gets the id and summary for a given document\n\n    Args:\n        id (int): document id\n\n    Returns:\n        pl.DataFrame: formatted polars df\n    \"\"\"\n    db = Database()\n    query = f\"\"\"\n            SELECT id, summary\n            FROM regulations_document\n            WHERE id = '{id}'\n            \"\"\"\n    schema = [\"id\", \"summary\"]\n    return pull_data(query=query, connection=db, schema=schema)\n</code></pre>"},{"location":"nlp/#civiclens.nlp.tools","title":"<code>tools</code>","text":""},{"location":"nlp/#civiclens.nlp.tools.Comment","title":"<code>Comment</code>","text":"Source code in <code>civiclens/nlp/tools.py</code> <pre><code>@dataclass\nclass Comment:\n    text: str = \"\"\n    num_represented: int = 1\n    id: str = str(uuid4())\n    topic_label: str = \"\"\n    topic: list[str] = None\n    form_letter: bool = False\n    sentiment: str = \"\"\n    source: str = \"Comment\"\n    representative: bool = False\n\n    def to_dict(self):\n        \"\"\"\n        Converts comment object to dictionary.\n        \"\"\"\n        return {\n            \"text\": self.text,\n            \"num_represented\": self.num_represented,\n            \"id\": self.id,\n            \"topic_label\": self.topic_label,\n            \"topic\": self.topic,\n            \"form_letter\": self.form_letter,\n            \"sentiment\": self.sentiment,\n            \"source\": self.source,\n        }\n</code></pre>"},{"location":"nlp/#civiclens.nlp.tools.Comment.to_dict","title":"<code>to_dict()</code>","text":"<p>Converts comment object to dictionary.</p> Source code in <code>civiclens/nlp/tools.py</code> <pre><code>def to_dict(self):\n    \"\"\"\n    Converts comment object to dictionary.\n    \"\"\"\n    return {\n        \"text\": self.text,\n        \"num_represented\": self.num_represented,\n        \"id\": self.id,\n        \"topic_label\": self.topic_label,\n        \"topic\": self.topic,\n        \"form_letter\": self.form_letter,\n        \"sentiment\": self.sentiment,\n        \"source\": self.source,\n    }\n</code></pre>"},{"location":"nlp/#civiclens.nlp.tools.RepComments","title":"<code>RepComments</code>","text":"Source code in <code>civiclens/nlp/tools.py</code> <pre><code>@dataclass(config=ConfigDict(arbitrary_types_allowed=True))\nclass RepComments:\n    # clustered df for topics\n    document_id: str\n    doc_comments: pl.DataFrame = Field(default=pl.DataFrame())\n\n    # fields for nlp table\n    rep_comments: list = Field(default=[])\n    doc_plain_english_title: str = \"\"\n    num_total_comments: int = 0\n    num_unique_comments: int = 0\n    num_representative_comment: int = 0\n    topics: list = Field(default=[])\n    last_updated: datetime = datetime.now()\n    uuid: int = uuid4().int\n    search_vector: list = Field(default=[])\n    summary: str = \"\"\n    representative: bool = True\n\n    # test this!\n    def get_nonrepresentative_comments(self):\n        \"\"\"\n        Converts nonrepresentative comments to list of Comment objects.\n        \"\"\"\n        rep_ids = set()\n        for comment in self.rep_comments:\n            if isinstance(comment, Comment):\n                rep_ids.add(comment.id)\n            else:\n                rep_ids.add(comment[\"comment_id\"])\n\n        return [\n            Comment(id=comment[\"id\"], text=comment[\"comment\"])\n            for comment in self.doc_comments.to_dicts()\n            if comment[\"id\"] not in rep_ids\n        ]\n\n    def to_list(self):\n        \"\"\"\n        Converts representative comments to list of Comment objects.\n        \"\"\"\n        if not self.rep_comments:\n            return []\n\n        return [\n            Comment(\n                text=comment[\"comment_text\"],\n                num_represented=comment[\"comments_represented\"],\n                id=comment[\"comment_id\"],\n                form_letter=comment[\"form_letter\"],\n                representative=True,\n            )\n            for comment in self.rep_comments\n        ]\n</code></pre>"},{"location":"nlp/#civiclens.nlp.tools.RepComments.get_nonrepresentative_comments","title":"<code>get_nonrepresentative_comments()</code>","text":"<p>Converts nonrepresentative comments to list of Comment objects.</p> Source code in <code>civiclens/nlp/tools.py</code> <pre><code>def get_nonrepresentative_comments(self):\n    \"\"\"\n    Converts nonrepresentative comments to list of Comment objects.\n    \"\"\"\n    rep_ids = set()\n    for comment in self.rep_comments:\n        if isinstance(comment, Comment):\n            rep_ids.add(comment.id)\n        else:\n            rep_ids.add(comment[\"comment_id\"])\n\n    return [\n        Comment(id=comment[\"id\"], text=comment[\"comment\"])\n        for comment in self.doc_comments.to_dicts()\n        if comment[\"id\"] not in rep_ids\n    ]\n</code></pre>"},{"location":"nlp/#civiclens.nlp.tools.RepComments.to_list","title":"<code>to_list()</code>","text":"<p>Converts representative comments to list of Comment objects.</p> Source code in <code>civiclens/nlp/tools.py</code> <pre><code>def to_list(self):\n    \"\"\"\n    Converts representative comments to list of Comment objects.\n    \"\"\"\n    if not self.rep_comments:\n        return []\n\n    return [\n        Comment(\n            text=comment[\"comment_text\"],\n            num_represented=comment[\"comments_represented\"],\n            id=comment[\"comment_id\"],\n            form_letter=comment[\"form_letter\"],\n            representative=True,\n        )\n        for comment in self.rep_comments\n    ]\n</code></pre>"},{"location":"nlp/#civiclens.nlp.tools.sentiment_analysis","title":"<code>sentiment_analysis(comment, pipeline)</code>","text":"<p>Analyze sentiment of a comment.</p> <p>Parameters:</p> Name Type Description Default <code>comment</code> <code>Comment</code> <p>Comment object</p> required <code>pipeline</code> <code>pipeline</code> <p>Hugging Face pipeline for conducting sentiment analysis</p> required <p>Returns:</p> Type Description <code>str</code> <p>Sentiment label as string (e.g 'postive', 'negative', 'neutral')</p> Source code in <code>civiclens/nlp/tools.py</code> <pre><code>def sentiment_analysis(comment: Comment, pipeline: pipeline) -&gt; str:\n    \"\"\"\n    Analyze sentiment of a comment.\n\n    Args:\n        comment: Comment object\n        pipeline: Hugging Face pipeline for conducting sentiment analysis\n\n    Returns:\n        Sentiment label as string (e.g 'postive', 'negative', 'neutral')\n    \"\"\"\n    try:\n        out = pipeline(comment.text)[0]\n    except Exception as e:\n        print(e)\n        return \"\"\n\n    out = pipeline(comment.text)[0]\n    return out[\"label\"]\n</code></pre>"},{"location":"nlp/#civiclens.nlp.topics","title":"<code>topics</code>","text":""},{"location":"nlp/#civiclens.nlp.topics.HDAModel","title":"<code>HDAModel</code>","text":"<p>Peforms LDA topic modeling</p> Source code in <code>civiclens/nlp/topics.py</code> <pre><code>class HDAModel:\n    \"\"\"\n    Peforms LDA topic modeling\n    \"\"\"\n\n    def __init__(self):\n        self.model = None\n        self.tokenizer = partial(regex_tokenize, pattern=r\"\\W+\")\n        self.stop_words = stopwords(\n            Path(__file__).resolve().parent / \"saved_models/stop_words.pickle\"\n        )\n        self.terms = None\n\n    def _process_text(\n        self, comments: list[Comment]\n    ) -&gt; tuple[list[list[str]], dict[int, str]]:\n        \"\"\"\n        Clean text and convert to tokens\n        \"\"\"\n        docs = []\n        document_ids = {}\n        for idx, comment in enumerate(comments):\n            docs.append(self.tokenizer(clean_text(comment.text).lower()))\n            document_ids[idx] = comment.id\n\n        # remove numbers, 2 character tokens, and stop words\n        docs = [\n            [\n                token\n                for token in doc\n                if not token.isnumeric()\n                and len(token) &gt; 2\n                and token not in self.stop_words\n            ]\n            for doc in docs\n        ]\n\n        return docs, document_ids\n\n    def _create_corpus(\n        self, docs: list[list[str]]\n    ) -&gt; tuple[Dictionary, list[tuple]]:\n        \"\"\"\n        Converts tokens to corpus and corresponding dictionary\n        \"\"\"\n        bigram_generator = Phrases(docs, min_count=10).freeze()\n\n        for doc in docs:\n            for token in bigram_generator[doc]:\n                if \"_\" in token:\n                    doc.append(token)\n\n        token_dict = corpora.Dictionary(docs)\n        corpus = [token_dict.doc2bow(doc) for doc in docs]\n\n        return token_dict, corpus\n\n    def run_model(self, comments: list[Comment]):\n        \"\"\"\n        Runs HDA topic analysis.\n        \"\"\"\n        docs, document_id = self._process_text(comments)\n        token_dict, corpus = self._create_corpus(docs)\n\n        hdp_model = HdpModel(corpus, token_dict)\n        numeric_topics = self._find_best_topic(hdp_model, corpus)\n\n        comment_topics = {}\n        topic_terms = {}\n        for doc_id, topic in numeric_topics.items():\n            comment_id = document_id[doc_id]\n            if topic not in topic_terms:\n                topic_terms[topic] = [\n                    word for word, _ in hdp_model.show_topic(topic)\n                ]\n            comment_topics[comment_id] = topic\n\n        self.terms = topic_terms\n\n        return comment_topics\n\n    def _find_best_topic(\n        self, model: HdpModel, corpus: list[tuple]\n    ) -&gt; dict[int, int]:\n        \"\"\"\n        Computes most probable topic per document\n        \"\"\"\n        best_topic = {}\n        for doc_id, doc in enumerate(corpus):\n            max_prob = float(\"-inf\")\n            topic_id = -1\n            for topic_num, prob in model[doc]:\n                if prob &gt; max_prob:\n                    max_prob = prob\n                    topic_id = topic_num\n            best_topic[doc_id] = topic_id\n\n        return best_topic\n\n    def get_terms(self) -&gt; dict:\n        \"\"\"\n        Returns terms for a all topics\n        \"\"\"\n        if not self.terms:\n            return {}\n\n        return self.terms\n\n    def generate_search_vector(self) -&gt; list[str]:\n        \"\"\"\n        Creates array of topics to use in Django serach model.\n        \"\"\"\n        if not self.terms:\n            raise RuntimeError(\n                \"Must run topic model before generating search vector\"\n            )\n\n        search_vector = set()\n        for term_list in self.terms.values():\n            search_vector.update(term_list)\n\n        return list(search_vector)\n</code></pre>"},{"location":"nlp/#civiclens.nlp.topics.HDAModel.generate_search_vector","title":"<code>generate_search_vector()</code>","text":"<p>Creates array of topics to use in Django serach model.</p> Source code in <code>civiclens/nlp/topics.py</code> <pre><code>def generate_search_vector(self) -&gt; list[str]:\n    \"\"\"\n    Creates array of topics to use in Django serach model.\n    \"\"\"\n    if not self.terms:\n        raise RuntimeError(\n            \"Must run topic model before generating search vector\"\n        )\n\n    search_vector = set()\n    for term_list in self.terms.values():\n        search_vector.update(term_list)\n\n    return list(search_vector)\n</code></pre>"},{"location":"nlp/#civiclens.nlp.topics.HDAModel.get_terms","title":"<code>get_terms()</code>","text":"<p>Returns terms for a all topics</p> Source code in <code>civiclens/nlp/topics.py</code> <pre><code>def get_terms(self) -&gt; dict:\n    \"\"\"\n    Returns terms for a all topics\n    \"\"\"\n    if not self.terms:\n        return {}\n\n    return self.terms\n</code></pre>"},{"location":"nlp/#civiclens.nlp.topics.HDAModel.run_model","title":"<code>run_model(comments)</code>","text":"<p>Runs HDA topic analysis.</p> Source code in <code>civiclens/nlp/topics.py</code> <pre><code>def run_model(self, comments: list[Comment]):\n    \"\"\"\n    Runs HDA topic analysis.\n    \"\"\"\n    docs, document_id = self._process_text(comments)\n    token_dict, corpus = self._create_corpus(docs)\n\n    hdp_model = HdpModel(corpus, token_dict)\n    numeric_topics = self._find_best_topic(hdp_model, corpus)\n\n    comment_topics = {}\n    topic_terms = {}\n    for doc_id, topic in numeric_topics.items():\n        comment_id = document_id[doc_id]\n        if topic not in topic_terms:\n            topic_terms[topic] = [\n                word for word, _ in hdp_model.show_topic(topic)\n            ]\n        comment_topics[comment_id] = topic\n\n    self.terms = topic_terms\n\n    return comment_topics\n</code></pre>"},{"location":"nlp/#civiclens.nlp.topics.LabelChain","title":"<code>LabelChain</code>","text":"Source code in <code>civiclens/nlp/topics.py</code> <pre><code>class LabelChain:\n    def __init__(self):\n        self.tokenizer = label_tokenizer\n        self.model = label_model\n\n    def generate_label(self, terms: list[str]) -&gt; tuple:\n        \"\"\"\n        Create better topic terms.\n        \"\"\"\n        text = \", \".join(terms)\n\n        inputs = self.tokenizer(\n            [text], max_length=512, truncation=True, return_tensors=\"pt\"\n        )\n        output = self.model.generate(\n            **inputs, num_beams=8, do_sample=True, min_length=10, max_length=64\n        )\n\n        decoded_output = self.tokenizer.batch_decode(\n            output, skip_special_tokens=True\n        )[0]\n\n        return tuple(set(decoded_output.strip().split(\", \")))\n</code></pre>"},{"location":"nlp/#civiclens.nlp.topics.LabelChain.generate_label","title":"<code>generate_label(terms)</code>","text":"<p>Create better topic terms.</p> Source code in <code>civiclens/nlp/topics.py</code> <pre><code>def generate_label(self, terms: list[str]) -&gt; tuple:\n    \"\"\"\n    Create better topic terms.\n    \"\"\"\n    text = \", \".join(terms)\n\n    inputs = self.tokenizer(\n        [text], max_length=512, truncation=True, return_tensors=\"pt\"\n    )\n    output = self.model.generate(\n        **inputs, num_beams=8, do_sample=True, min_length=10, max_length=64\n    )\n\n    decoded_output = self.tokenizer.batch_decode(\n        output, skip_special_tokens=True\n    )[0]\n\n    return tuple(set(decoded_output.strip().split(\", \")))\n</code></pre>"},{"location":"nlp/#civiclens.nlp.topics.create_topics","title":"<code>create_topics(comments)</code>","text":"<p>Condense topics for document summary</p> <p>Parameters:</p> Name Type Description Default <code>Comments</code> <p>list of Comment objects</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of topics, and corresponding sentiment data</p> Source code in <code>civiclens/nlp/topics.py</code> <pre><code>def create_topics(comments: list[Comment]) -&gt; dict:\n    \"\"\"\n    Condense topics for document summary\n\n    Args:\n        Comments: list of Comment objects\n\n    Returns:\n        Dictionary of topics, and corresponding sentiment data\n    \"\"\"\n    temp = defaultdict(dict)\n\n    for comment in comments:\n        temp[comment.topic_label][comment.sentiment] = (\n            temp[comment.topic_label].get(comment.sentiment, 0)\n            + comment.num_represented\n        )\n        temp[comment.topic_label][\"total\"] = (\n            temp[comment.topic_label].get(\"total\", 0) + comment.num_represented\n        )\n\n    topics = []\n    # create output dictionary\n    for topic_label, part in temp.items():\n        part[\"topic\"] = topic_label\n        topics.append(part)\n\n    # sort topics by \"total\"\n    return sorted(topics, key=lambda topic: topic[\"total\"], reverse=True)\n</code></pre>"},{"location":"nlp/#civiclens.nlp.topics.label_topics","title":"<code>label_topics(topics, model)</code>","text":"<p>Generates a label for all topics</p> <p>Parameters:</p> Name Type Description Default <code>topics</code> <code>dict[int, list]</code> <p>dictionary of topics, as lists of terms</p> required <code>model</code> <code>LabelChain</code> <p>LLM model to generate labels</p> required <p>Returns:</p> Type Description <code>dict[int, str]</code> <p>Dictionary of topics, and labels</p> Source code in <code>civiclens/nlp/topics.py</code> <pre><code>def label_topics(topics: dict[int, list], model: LabelChain) -&gt; dict[int, str]:\n    \"\"\"\n    Generates a label for all topics\n\n    Args:\n        topics: dictionary of topics, as lists of terms\n        model: LLM model to generate labels\n\n    Returns:\n        Dictionary of topics, and labels\n    \"\"\"\n    labels = {}\n    for topic, terms in topics.items():\n        labels[topic] = model.generate_label(terms)\n\n    return labels\n</code></pre>"},{"location":"nlp/#civiclens.nlp.topics.stopwords","title":"<code>stopwords(model_path)</code>","text":"<p>Loads in pickled set of stopword for text processing.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Path</code> <p>path from downloaded model</p> required <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of stop words.</p> Source code in <code>civiclens/nlp/topics.py</code> <pre><code>def stopwords(model_path: Path) -&gt; set[str]:\n    \"\"\"\n    Loads in pickled set of stopword for text processing.\n\n    Args:\n        model_path: path from downloaded model\n\n    Returns:\n        Set of stop words.\n    \"\"\"\n    with open(model_path, \"rb\") as f:\n        stop_words = pickle.load(f)\n\n    return stop_words\n</code></pre>"},{"location":"nlp/#civiclens.nlp.topics.topic_comment_analysis","title":"<code>topic_comment_analysis(comment_data, model=None, labeler=None, sentiment_analyzer=None)</code>","text":"<p>Run topic and sentiment analysis.</p> <p>Parameters:</p> Name Type Description Default <code>comment_data</code> <code>RepComments</code> <p>RepComment object</p> required <code>model</code> <code>HDAModel</code> <p>instance topic model class</p> <code>None</code> <code>labeler</code> <code>LabelChain</code> <p>chain for generating topic labels</p> <code>None</code> <code>sentiment_analyzer</code> <code>Callable</code> <p>function to analyze comment text sentiment</p> <code>None</code> <p>Returns:</p> Type Description <code>RepComments</code> <p>RepComment object with full topic anlayis complete</p> Source code in <code>civiclens/nlp/topics.py</code> <pre><code>def topic_comment_analysis(\n    comment_data: RepComments,\n    model: HDAModel = None,\n    labeler: LabelChain = None,\n    sentiment_analyzer: Callable = None,\n) -&gt; RepComments:\n    \"\"\"\n    Run topic and sentiment analysis.\n\n    Args:\n        comment_data: RepComment object\n        model: instance topic model class\n        labeler: chain for generating topic labels\n        sentiment_analyzer: function to analyze comment text sentiment\n\n    Returns:\n        RepComment object with full topic anlayis complete\n    \"\"\"\n    comments: list[Comment] = []\n\n    if comment_data.summary:\n        comments += [\n            Comment(text=comment_data.summary, id=\"Summary\", source=\"Summary\")\n        ]\n\n    comments += comment_data.to_list()\n    if not comment_data.rep_comments:\n        comment_data.representative = False\n        comments += comment_data.get_nonrepresentative_comments()\n\n    comment_topics = model.run_model(comments)\n    topic_terms = model.get_terms()\n    topic_labels = label_topics(topic_terms, labeler)\n\n    # filter out non_rep comments\n    rep_comments: list[Comment] = []\n\n    for comment in comments:\n        comment.topic_label = topic_labels[comment_topics[comment.id]]\n        comment.topic = comment_topics[comment.id]\n        comment.sentiment = sentiment_analyzer(comment)\n        if comment.representative or not comment_data.representative:\n            rep_comments.append(comment)\n\n    rep_comments = sorted(\n        rep_comments, key=lambda comment: comment.num_represented, reverse=True\n    )\n\n    return RepComments(\n        document_id=comment_data.document_id,\n        doc_comments=comment_data.doc_comments,\n        rep_comments=[comment.to_dict() for comment in rep_comments],\n        doc_plain_english_title=comment_data.doc_plain_english_title,\n        num_total_comments=comment_data.num_total_comments,\n        num_unique_comments=comment_data.num_unique_comments,\n        num_representative_comment=comment_data.num_representative_comment,\n        topics=create_topics(comments),\n        search_vector=model.generate_search_vector(),\n        representative=comment_data.representative,\n    )\n</code></pre>"},{"location":"nlp/#civiclens.utils.text","title":"<code>text</code>","text":""},{"location":"nlp/#civiclens.utils.text.clean_text","title":"<code>clean_text(text, patterns=None)</code>","text":"<p>String cleaning function for comments.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>comment text</p> required <code>patterns</code> <code>list[str]</code> <p>optional list of regular expression patterns to pass in (eg. [(r'\\w+', \"-\")])</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Cleaned verison of text</p> Source code in <code>civiclens/utils/text.py</code> <pre><code>def clean_text(text: str, patterns: Optional[list[tuple]] = None) -&gt; str:\n    r\"\"\"\n    String cleaning function for comments.\n\n    Args:\n        text (str): comment text\n        patterns (list[str]): optional list of regular expression patterns\n            to pass in (eg. [(r'\\w+', \"-\")])\n\n    Returns:\n        Cleaned verison of text\n    \"\"\"\n    if patterns is None:\n        patterns = []\n\n    text = re.sub(r\"&amp;#39;\", \"'\", text)  # this replaces html entity with '\n    text = re.sub(r\"&amp;rdquo;\", '\"', text)  # this replaces html entity with \"\n    text = re.sub(r\"&amp;amp;\", \"&amp;\", text)  # this replaces html entity with &amp;\n    text = re.sub(r\"\u00e2\", \"\", text)\n    text = re.sub(r\"&lt;br\\s*/?&gt;\", \"\", text)\n\n    text = re.sub(r\"&lt;\\s*br\\s*/&gt;\", \" \", text)\n    text = re.sub(r\"[^a-zA-Z0-9.'\\\"\\?\\: -]\", \"\", text)\n    text = re.sub(r\"\\w*ndash\\w*\", \"\", text)\n\n    if patterns:\n        for pattern, replacement in patterns:\n            text = re.sub(pattern, replacement, text)\n\n    # remove extra whitespace\n    return re.sub(r\"\\s+\", \" \", text).strip()\n</code></pre>"},{"location":"nlp/#civiclens.utils.text.regex_tokenize","title":"<code>regex_tokenize(text, pattern='\\\\W+')</code>","text":"<p>Splits strings into tokens base on regular expression.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>string to tokenize</p> required <code>pattern</code> <code>str</code> <p>regular expression to split tokens on, defaults to white space</p> <code>'\\\\W+'</code> <p>Returns:</p> Type Description <p>List of strings represented tokens</p> Source code in <code>civiclens/utils/text.py</code> <pre><code>def regex_tokenize(text: str, pattern: str = r\"\\W+\"):\n    \"\"\"\n    Splits strings into tokens base on regular expression.\n\n    Args:\n        text: string to tokenize\n        pattern: regular expression to split tokens on, defaults to white space\n\n    Returns:\n        List of strings represented tokens\n    \"\"\"\n    return re.split(pattern, text)\n</code></pre>"},{"location":"nlp/#civiclens.utils.text.sentence_splitter","title":"<code>sentence_splitter(text, sep='.')</code>","text":"<p>Splits string into sentences.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>string to process</p> required <code>sep</code> <code>str</code> <p>value to seperate string on, defaults to '.'</p> <code>'.'</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of strings split on the seperator valur</p> Source code in <code>civiclens/utils/text.py</code> <pre><code>def sentence_splitter(text: str, sep: str = \".\") -&gt; list[str]:\n    \"\"\"\n    Splits string into sentences.\n\n    Args:\n        text: string to process\n        sep: value to seperate string on, defaults to '.'\n\n    Returns:\n        List of strings split on the seperator valur\n    \"\"\"\n    # remove periods not at the end of sentences\n    clean = re.sub(r\"\\.(?!\\s)\", \" \", text)\n    sentences = clean.split(sep)\n\n    return [sentence.strip() + \".\" for sentence in sentences if sentence]\n</code></pre>"},{"location":"nlp/#civiclens.utils.text.truncate","title":"<code>truncate(text, num_words)</code>","text":"<p>Truncates commments:</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text of the comment</p> required <code>num_words</code> <code>int</code> <p>Number of words to keep</p> required <p>Returns:</p> Type Description <code>str</code> <p>Truncated commented</p> Source code in <code>civiclens/utils/text.py</code> <pre><code>def truncate(text: str, num_words: int) -&gt; str:\n    \"\"\"\n    Truncates commments:\n\n    Args:\n        text (str): Text of the comment\n        num_words (int): Number of words to keep\n\n    Returns:\n        Truncated commented\n    \"\"\"\n    words = text.split(\" \")\n\n    return \" \".join(words[:num_words])\n</code></pre>"},{"location":"nlp/#civiclens.utils.database_access","title":"<code>database_access</code>","text":""},{"location":"nlp/#civiclens.utils.database_access.Database","title":"<code>Database</code>","text":"<p>Wrapper for CivicLens postrgres DB.</p> Source code in <code>civiclens/utils/database_access.py</code> <pre><code>class Database:\n    \"\"\"\n    Wrapper for CivicLens postrgres DB.\n    \"\"\"\n\n    def __init__(self):\n        self.conn = psycopg2.connect(\n            database=os.getenv(\"DATABASE\"),\n            user=os.getenv(\"DATABASE_USER\"),\n            password=os.getenv(\"DATABASE_PASSWORD\"),\n            host=os.getenv(\"DATABASE_HOST\"),\n            port=os.getenv(\"DATABASE_PORT\"),\n        )\n\n    def cursor(self):\n        return self.conn.cursor()\n\n    def close(self):\n        return self.conn.close()\n\n    def commit(self):\n        return self.conn.commit()\n</code></pre>"},{"location":"nlp/#civiclens.utils.database_access.pull_data","title":"<code>pull_data(connection, query, schema=None, return_type='df')</code>","text":"<p>Takes a SQL Query and returns a polars dataframe</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL Query</p> required <code>schema</code> <code>list[str]</code> <p>list of column names for the dataframe</p> <code>None</code> <code>return_type</code> <code>str</code> <p>\"df\" or \"list\"</p> <code>'df'</code> <p>Returns:</p> Type Description <code>DataFrame | List[Tuple]</code> <p>Polars df of comment data or list of comment data</p> Source code in <code>civiclens/utils/database_access.py</code> <pre><code>def pull_data(\n    connection: Database,\n    query: str,\n    schema: Optional[List[str]] = None,\n    return_type: str = \"df\",\n) -&gt; pl.DataFrame | List[Tuple]:\n    \"\"\"Takes a SQL Query and returns a polars dataframe\n\n    Args:\n        query (str): SQL Query\n        schema (list[str]): list of column names for the dataframe\n        return_type (str): \"df\" or \"list\"\n\n    Returns:\n        Polars df of comment data or list of comment data\n    \"\"\" \"\"\"\"\"\"\n    if return_type == \"df\" and not schema:\n        raise ValueError(\"Must input schema to return df\")\n\n    try:\n        cursor = connection.cursor()\n        cursor.execute(query)\n        results = cursor.fetchall()\n    except (Exception, psycopg2.Error) as error:\n        raise RuntimeError(\n            f\"Error while connecting to PostgreSQL: {str(error).strip()}\"\n        ) from error\n\n    finally:\n        # Close the connection and cursor to free resources\n        if connection:\n            cursor.close()\n            connection.close()\n            print(\"PostgreSQL connection is closed\")\n\n    if return_type == \"df\":\n        results = pl.DataFrame(results, schema=schema)\n\n    return results\n</code></pre>"},{"location":"nlp/#civiclens.utils.database_access.upload_comments","title":"<code>upload_comments(connection, comments)</code>","text":"<p>Uploads comment data to database.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Database</code> <p>Postgres client</p> required <code>comments</code> <code>RepComments</code> <p>comments to be uploaded</p> required <p>Returns:</p> Type Description <code>None</code> <p>None, uploads comments to database</p> Source code in <code>civiclens/utils/database_access.py</code> <pre><code>def upload_comments(connection: Database, comments: RepComments) -&gt; None:\n    \"\"\"\n    Uploads comment data to database.\n\n    Args:\n        connection: Postgres client\n        comments: comments to be uploaded\n\n    Returns:\n        None, uploads comments to database\n    \"\"\"\n    query = \"\"\"\n    INSERT INTO regulations_nlpoutput (\n            comments,\n            is_representative,\n            doc_plain_english_title,\n            num_total_comments,\n            num_unique_comments,\n            num_representative_comment,\n            topics,\n            num_topics,\n            last_updated,\n            created_at,\n            search_topics,\n            document_id)\n        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n        ON CONFLICT (document_id)\n        DO UPDATE SET\n            comments = EXCLUDED.comments,\n            is_representative = EXCLUDED.is_representative,\n            doc_plain_english_title = EXCLUDED.doc_plain_english_title,\n            num_total_comments = EXCLUDED.num_total_comments,\n            num_unique_comments = EXCLUDED.num_unique_comments,\n            num_representative_comment = EXCLUDED.num_representative_comment,\n            topics = EXCLUDED.topics,\n            num_topics = EXCLUDED.num_topics,\n            last_updated = NOW(),\n            search_topics = EXCLUDED.search_topics\n        WHERE regulations_nlpoutput.last_updated IS NULL\n            OR regulations_nlpoutput.last_updated &lt; EXCLUDED.last_updated;\n            \"\"\"\n\n    values = (\n        json.dumps(comments.rep_comments),\n        comments.representative,\n        comments.doc_plain_english_title,\n        comments.num_total_comments,\n        comments.num_unique_comments,\n        comments.num_representative_comment,\n        json.dumps(comments.topics),\n        len(comments.topics),\n        datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"),\n        comments.last_updated.strftime(\"%m/%d/%Y, %H:%M:%S\"),\n        \", \".join(comments.search_vector),\n        comments.document_id,\n    )\n\n    try:\n        cursor = connection.cursor()\n        cursor.execute(query, values)\n        connection.commit()\n\n    except Exception as e:\n        print(e)\n\n    if connection:\n        cursor.close()\n        connection.close()\n        print(\"PostgreSQL connection is closed\")\n</code></pre>"},{"location":"endpoints/about/","title":"About","text":"<p>/about</p> <p>This page outlines the goal of CivicLens, which is to make public commenting on government regulations more accessible and understandable. It also outlines the methodology of the site, including a detailed description of how it pulls data nightly from regulations.gov API, improves search functionality and provides meaningful analysis on existing comments, and uses AI to generate readable document titles and identify unique/form letters, representative comments, and sentiment.</p> <p>Parameters: None.</p> <p>Response: HTML page displaying the motivator behind the project, our methodology, and bios for the developers of the project.</p>"},{"location":"endpoints/document/","title":"Document","text":"<p>/document/{str:document_id}</p> <p>Displays plain English title, posted document title, plain english summary of proposed rule, representative comments, and metadata (posting agency, dates of comment period, number of comments). It alsos display visualization of sentiment analysis of comments broken down by topic.</p> <p>Parameters: None</p> <p>Example: <code>/document/ATF-2024-0001-0001</code></p> <p>Response: HTML page displaying fixed text (title, summary). D3 visual displaying sentiment analysis. Cards displaying representative comments. Hyperlinks out to other government websites with useful information about the opportunity to comment.</p>"},{"location":"endpoints/home/","title":"Home","text":"<p>/</p> <p>Homepage for CivicLens. Offers basic description of the site and buttons for the core functionality of the site:</p> <ul> <li>learning more about public commenting, and</li> <li>searching for opportunities to comment.</li> </ul> <p>Parameters: None.</p> <p>Response: HTML page displaying navigational links and a brief overview of the why we built our project.</p>"},{"location":"endpoints/learn/","title":"Learn","text":"<p>/learn</p> <p>The purpose of the page is to explain the public commenting process on proposed government regulations, detailing how individuals and organizations can participate and the impact their feedback can have.</p> <p>It outlines the four main steps of the process: drafting rules, commenting during a public period, agency consideration of feedback, and finalizing the rules. We want users to get a better sense of the process and get all the information they need to comment themselves.</p> <p>Parameters: None.</p> <p>Response: HTML page displaying educational content about public commenting.</p>"},{"location":"endpoints/search/","title":"Search","text":"<p>/search</p> <p>Initial search page. Provides search bar and showcases trending topics or recently uploaded documents for users to click on.</p> <p>Parameters:</p> <ul> <li>topics: list of trending topics to display</li> </ul> <p>Example: <code>/search/?topics=gun_control+abortion+covid-19+healthcare</code></p> <p>Response: HTML page with search bar and trending topics.</p> <p>/search_result</p> <p>Shows search results, ways to sort and filter results, along with search bar for user to enter new search terms.</p> <p>Parameters:</p> <ul> <li>sort: by_date, by_num_comments (order number of posted comments)</li> <li>filter:<ul> <li>by_agency (eg. FDA, HUD)</li> <li>if_comment (whether a document has comment posted to it)</li> <li>doc_type (All, Notice, Proposed Rule, etc)</li> </ul> </li> </ul> <p>Example: <code>/search_result/?sort=by_date&amp;by_agency=FDA&amp;if_comment=true&amp;doc_type=all</code></p> <p>Response: HTML page with search bar and search results (list of documents). Infinitive scroll container for displaying resulting documents.</p>"}]}