{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import json\n",
    "from bertopic import BERTopic\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, TextGeneration\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from hdbscan import HDBSCAN\n",
    "from lxml import etree\n",
    "import regex as re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule text\n",
    "resp = requests.get(\"https://www.federalregister.gov/documents/full_text/xml/2024/04/10/2024-06218.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(element):\n",
    "    text = element.text.strip() if element.text else ''\n",
    "    for child in element:\n",
    "        text += get_text(child)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_text = re.sub(r'<[^<>]+>', '', resp.text)\n",
    "rule_text = re.sub(r'\\n|\\s{2,}', ' ', rule_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"This proposed rule would amend the regulations for certain HUD Public and Indian Housing and Housing Programs. The proposed amendments would revise existing regulations that govern admission for applicants with criminal records or a history of involvement with the criminal justice system and eviction or termination of assistance of persons on the basis of illegal drug use, drug-related criminal activity, or other criminal activity. The proposed revisions would require that prior to any discretionary denial or termination for criminal activity, PHAs and assisted housing owners take into consideration multiple sources of information, including but not limited to the recency and relevance of prior criminal activity. They are intended to minimize unnecessary exclusions from these programs while allowing providers to maintain the health, safety, and peaceful enjoyment of their residents, their staffs, and their communities. The proposed rule is intended to both clarify existing PHA and owner obligations and reduce the risk of violation of nondiscrimination laws.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../HUD-2024-0031-0001_comments.json\", \"r\") as f:\n",
    "    comments = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_text = []\n",
    "\n",
    "for comment in comments:\n",
    "    comment_text.append(comment['data']['attributes']['comment'])\n",
    "\n",
    "# remove empty comment\n",
    "docs = [comment for comment in comment_text if comment]\n",
    "sentences = [sent_tokenize(comment) for comment in docs]\n",
    "sentences = [sentence for doc in sentences for sentence in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdbscan_model = HDBSCAN(min_cluster_size=150, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))\n",
    "\n",
    "keybert_model = KeyBERTInspired()\n",
    "# # MMR\n",
    "# mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Here is a proposed rule change from the federal government: {summary}\n",
    "The following documents represent public comments on this rule: [DOCUMENTS] \n",
    "Topics from these documents are described by the following keywords: [KEYWORDS]\n",
    "\n",
    "Take these topics and create a single word label for each grouping.\n",
    "\"\"\"\n",
    "tokenizer_kwargs = {'truncation': True, 'max_length': 512}\n",
    "\n",
    "generator = pipeline('text2text-generation', tokenizer=tokenizer, model=model, **tokenizer_kwargs)\n",
    "flan_model = TextGeneration(generator, prompt=prompt, nr_docs=12)\n",
    "\n",
    "models = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    # \"MMR\": mmr_model,\n",
    "    \"Flan\": flan_model\n",
    "}\n",
    "\n",
    "topic_model = BERTopic(vectorizer_model=vectorizer_model, representation_model=models)\n",
    "topics, probs = topic_model.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>Flan</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>860</td>\n",
       "      <td>0_housing_criminal_people_individuals</td>\n",
       "      <td>[housing, criminal, people, individuals, acces...</td>\n",
       "      <td>[hud housing, ensure criminal, criminal record...</td>\n",
       "      <td>[Housing is a human right, but too many people...</td>\n",
       "      <td>[&lt;br/&gt;&amp;nbsp;&lt;br/&gt;Housing is a human right, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>1_tenancy_outcomes_evidence_housing makes</td>\n",
       "      <td>[tenancy, outcomes, evidence, housing makes, c...</td>\n",
       "      <td>[criminal record, criminal records, conviction...</td>\n",
       "      <td>[br/&gt;br/&gt;A criminal record has no bearing on t...</td>\n",
       "      <td>[&lt;br/&gt;&lt;br/&gt;A criminal record has no bearing on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>2_adopt lookback_hud adopt_urge hud_period years</td>\n",
       "      <td>[adopt lookback, hud adopt, urge hud, period y...</td>\n",
       "      <td>[hud, hud adopt, lookback period, years regula...</td>\n",
       "      <td>[i urge hud to adopt a lookback period of no m...</td>\n",
       "      <td>[&lt;br/&gt;&lt;br/&gt;Further, I urge HUD to adopt a look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>3_fair chance_chance housing_fair_chance</td>\n",
       "      <td>[fair chance, chance housing, fair, chance, in...</td>\n",
       "      <td>[individualized review, public housing, housin...</td>\n",
       "      <td>[individualized review process will give peopl...</td>\n",
       "      <td>[The individualized review process will give p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>4_removed_contact_manner_criminal legal</td>\n",
       "      <td>[removed, contact, manner, criminal legal, tre...</td>\n",
       "      <td>[lookback period, lookback, background checks,...</td>\n",
       "      <td>[importantly, a lookback period will ensure th...</td>\n",
       "      <td>[Importantly, a lookback period will ensure th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>5_dignity_millions people_right direction_dign...</td>\n",
       "      <td>[dignity, millions people, right direction, di...</td>\n",
       "      <td>[lookback period, lookback, human rights, rest...</td>\n",
       "      <td>[Enforcing a lookback period is a move in the ...</td>\n",
       "      <td>[Enforcing a lookback period is a move in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "      <td>6_surface_screening_biases_housing providers</td>\n",
       "      <td>[surface, screening, biases, housing providers...</td>\n",
       "      <td>[lookback period, lookback, screening, individ...</td>\n",
       "      <td>[Requiring a lookback period will ensure unifo...</td>\n",
       "      <td>[Requiring a lookback period will ensure unifo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>7_increase housing_effects increase_homelessne...</td>\n",
       "      <td>[increase housing, effects increase, homelessn...</td>\n",
       "      <td>[barriers housing, homelessness decrease, hous...</td>\n",
       "      <td>[In fact, barriers to housing have negative ef...</td>\n",
       "      <td>[In fact, barriers to housing have negative ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>8_right people_need criminal_access fundamenta...</td>\n",
       "      <td>[right people, need criminal, access fundament...</td>\n",
       "      <td>[hud housing, people conviction, ensure crimin...</td>\n",
       "      <td>[I support HUD&amp;rsquo;s proposed regulations th...</td>\n",
       "      <td>[Housing is a human right, but too many people...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>9_negative_housing negative_negative effects_p...</td>\n",
       "      <td>[negative, housing negative, negative effects,...</td>\n",
       "      <td>[barriers housing, homelessness decrease, hous...</td>\n",
       "      <td>[In fact, barriers to housing have negative ef...</td>\n",
       "      <td>[In fact, barriers to housing have negative ef...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                               Name  \\\n",
       "0      0    860              0_housing_criminal_people_individuals   \n",
       "1      1     56          1_tenancy_outcomes_evidence_housing makes   \n",
       "2      2     56   2_adopt lookback_hud adopt_urge hud_period years   \n",
       "3      3     54           3_fair chance_chance housing_fair_chance   \n",
       "4      4     50            4_removed_contact_manner_criminal legal   \n",
       "5      5     48  5_dignity_millions people_right direction_dign...   \n",
       "6      6     47       6_surface_screening_biases_housing providers   \n",
       "7      7     33  7_increase housing_effects increase_homelessne...   \n",
       "8      8     22  8_right people_need criminal_access fundamenta...   \n",
       "9      9     18  9_negative_housing negative_negative effects_p...   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [housing, criminal, people, individuals, acces...   \n",
       "1  [tenancy, outcomes, evidence, housing makes, c...   \n",
       "2  [adopt lookback, hud adopt, urge hud, period y...   \n",
       "3  [fair chance, chance housing, fair, chance, in...   \n",
       "4  [removed, contact, manner, criminal legal, tre...   \n",
       "5  [dignity, millions people, right direction, di...   \n",
       "6  [surface, screening, biases, housing providers...   \n",
       "7  [increase housing, effects increase, homelessn...   \n",
       "8  [right people, need criminal, access fundament...   \n",
       "9  [negative, housing negative, negative effects,...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [hud housing, ensure criminal, criminal record...   \n",
       "1  [criminal record, criminal records, conviction...   \n",
       "2  [hud, hud adopt, lookback period, years regula...   \n",
       "3  [individualized review, public housing, housin...   \n",
       "4  [lookback period, lookback, background checks,...   \n",
       "5  [lookback period, lookback, human rights, rest...   \n",
       "6  [lookback period, lookback, screening, individ...   \n",
       "7  [barriers housing, homelessness decrease, hous...   \n",
       "8  [hud housing, people conviction, ensure crimin...   \n",
       "9  [barriers housing, homelessness decrease, hous...   \n",
       "\n",
       "                                                Flan  \\\n",
       "0  [Housing is a human right, but too many people...   \n",
       "1  [br/>br/>A criminal record has no bearing on t...   \n",
       "2  [i urge hud to adopt a lookback period of no m...   \n",
       "3  [individualized review process will give peopl...   \n",
       "4  [importantly, a lookback period will ensure th...   \n",
       "5  [Enforcing a lookback period is a move in the ...   \n",
       "6  [Requiring a lookback period will ensure unifo...   \n",
       "7  [In fact, barriers to housing have negative ef...   \n",
       "8  [I support HUD&rsquo;s proposed regulations th...   \n",
       "9  [In fact, barriers to housing have negative ef...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [<br/>&nbsp;<br/>Housing is a human right, but...  \n",
       "1  [<br/><br/>A criminal record has no bearing on...  \n",
       "2  [<br/><br/>Further, I urge HUD to adopt a look...  \n",
       "3  [The individualized review process will give p...  \n",
       "4  [Importantly, a lookback period will ensure th...  \n",
       "5  [Enforcing a lookback period is a move in the ...  \n",
       "6  [Requiring a lookback period will ensure unifo...  \n",
       "7  [In fact, barriers to housing have negative ef...  \n",
       "8  [Housing is a human right, but too many people...  \n",
       "9  [In fact, barriers to housing have negative ef...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llama = Llama.from_pretrained(\n",
    "    repo_id=\"TheBloke/zephyr-7B-alpha-GGUF\",\n",
    "    filename=\"zephyr-7b-alpha.Q4_K_M.gguf\",\n",
    "    n_ctx=4096,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef26b1942d547f49e43841ca8fda195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86aaa9d2757b4911b2a2f87b381543db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a6ca25c78d4ba7ba902a2d7a953609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a3b087f1c14bd6b6ce8b243b4a69c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1920fe6792c845a69882ef8cc5ef4774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(sentences, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([    1,   382, 23291,   349,   264,  2930,  1103, 28723]),\n",
       "       array([    1,  1387, 27440, 28770, 28774, 28745, 28713,   708, 13406,\n",
       "                970,   478,   541,  3317,   354,   312, 23005,  5174,  1671,\n",
       "               7501,   272,  5537,   298,  2647, 11854,  3208, 28725,  2490,\n",
       "              11821, 28723])                                                ,\n",
       "       array([    1,  5671,   283,  2742,   352,   704,   729,   483,   296,\n",
       "               1999, 26336,  4777, 28725,  7402,  2164, 28725,  1756, 20164,\n",
       "               5619, 28725,   799,   905,   302,  3181, 28725,   905,   395,\n",
       "                704,  7773, 28725,   304,   799,  4264,  1944, 25286,  1332,\n",
       "                304, 23759,  8065,  9750, 28725,   690, 26336,  1012,  3252,\n",
       "                299,   302,   652,  1411, 28725,   304,  1671, 11854,  3208,\n",
       "                737,  2887,   304, 11821,  2735, 28725,   272, 13290,   302,\n",
       "                937,   313,   449,  1443,   460, 15702,  7483,   304,   264,\n",
       "                363, 10573, 10061,   349,  3859,   369,   349, 18894,   346,\n",
       "                304, 25340,   346,  3796,   298,  5281,  4413,   599,   477,\n",
       "              28723])                                                       ,\n",
       "       ...,\n",
       "       array([    1,   523,  1473,  2370, 28789,  1473,  2370,  2324,   506,\n",
       "                750, 25475,   304,  5502,   286, 20151, 28723])             ,\n",
       "       array([    1,  5919,  2968,   684,  1188, 28733, 14464,   286,  2268,\n",
       "              28723])                                                       ,\n",
       "       array([    1,  8868,   354,   574,   727,   304, 15301,   302,  1167,\n",
       "               7616, 26364,  1473,  2370, 28789,  1473,  2370, 28735,  1814,\n",
       "                263,   723, 28725, 28789,  1473,  2370, 28789,  1473,  2370,\n",
       "              28757,   280, 28721, 11578,  1822, 28789,  1473,  2370, 28789,\n",
       "               1473,  2370, 28769,   858,   266, 28725, 23579,   523,  1473,\n",
       "               2370, 28789,  1473,  2370])                                  ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_set in tokens['input_ids']:\n",
    "    if len(token_set) >= 512:\n",
    "        print('too long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import LlamaCPP\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Q: Here is a proposed rule change from the federal government: {summary}\n",
    "The following documents represent public comments on this rule: \n",
    "[DOCUMENTS] \n",
    "\n",
    "The topic is described by the following keywords: '[KEYWORDS]'.\n",
    "\n",
    "Based on the above information, can you give a short label of the topic?\n",
    "A: \n",
    "\"\"\"\n",
    "llama_args = {\"max_tokens\" : 512}\n",
    "representation_model = LlamaCPP(llama, prompt=prompt)\n",
    "topic_model = BERTopic(representation_model=representation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m topics, probs \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/bertopic/_bertopic.py:433\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_representative_docs(custom_documents)\n\u001b[1;32m    431\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m     \u001b[39m# Extract topics by calculating c-TF-IDF\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_topics(documents, embeddings\u001b[39m=\u001b[39;49membeddings, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose)\n\u001b[1;32m    435\u001b[0m     \u001b[39m# Reduce topics\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnr_topics:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/bertopic/_bertopic.py:3637\u001b[0m, in \u001b[0;36mBERTopic._extract_topics\u001b[0;34m(self, documents, embeddings, mappings, verbose)\u001b[0m\n\u001b[1;32m   3635\u001b[0m documents_per_topic \u001b[39m=\u001b[39m documents\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mTopic\u001b[39m\u001b[39m'\u001b[39m], as_index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39mDocument\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin})\n\u001b[1;32m   3636\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_tf_idf_, words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_tf_idf(documents_per_topic)\n\u001b[0;32m-> 3637\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_representations_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_words_per_topic(words, documents)\n\u001b[1;32m   3638\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_topic_vectors(documents\u001b[39m=\u001b[39mdocuments, embeddings\u001b[39m=\u001b[39membeddings, mappings\u001b[39m=\u001b[39mmappings)\n\u001b[1;32m   3639\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_labels_ \u001b[39m=\u001b[39m {key: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([word[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m values[:\u001b[39m4\u001b[39m]])\n\u001b[1;32m   3640\u001b[0m                       \u001b[39mfor\u001b[39;00m key, values \u001b[39min\u001b[39;00m\n\u001b[1;32m   3641\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_representations_\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/bertopic/_bertopic.py:3922\u001b[0m, in \u001b[0;36mBERTopic._extract_words_per_topic\u001b[0;34m(self, words, documents, c_tf_idf, calculate_aspects)\u001b[0m\n\u001b[1;32m   3920\u001b[0m         topics \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mextract_topics(\u001b[39mself\u001b[39m, documents, c_tf_idf, topics)\n\u001b[1;32m   3921\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentation_model, BaseRepresentation):\n\u001b[0;32m-> 3922\u001b[0m     topics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepresentation_model\u001b[39m.\u001b[39;49mextract_topics(\u001b[39mself\u001b[39;49m, documents, c_tf_idf, topics)\n\u001b[1;32m   3923\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentation_model, \u001b[39mdict\u001b[39m):\n\u001b[1;32m   3924\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentation_model\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mMain\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/bertopic/representation/_llamacpp.py:154\u001b[0m, in \u001b[0;36mLlamaCPP.extract_topics\u001b[0;34m(self, topic_model, documents, c_tf_idf, topics)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompts_\u001b[39m.\u001b[39mappend(prompt)\n\u001b[1;32m    153\u001b[0m \u001b[39m# Extract result from generator and use that as label\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m topic_description \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(prompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpipeline_kwargs)[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    155\u001b[0m topic_description \u001b[39m=\u001b[39m [(description[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mreplace(prompt, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m description \u001b[39min\u001b[39;00m topic_description]\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(topic_description) \u001b[39m<\u001b[39m \u001b[39m10\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/llama_cpp/llama.py:1561\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m   1498\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1499\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1523\u001b[0m     logit_bias: Optional[Dict[\u001b[39mstr\u001b[39m, \u001b[39mfloat\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1524\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[1;32m   1525\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m \n\u001b[1;32m   1527\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[39m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1561\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_completion(\n\u001b[1;32m   1562\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m   1563\u001b[0m         suffix\u001b[39m=\u001b[39;49msuffix,\n\u001b[1;32m   1564\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m   1565\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m   1566\u001b[0m         top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m   1567\u001b[0m         min_p\u001b[39m=\u001b[39;49mmin_p,\n\u001b[1;32m   1568\u001b[0m         typical_p\u001b[39m=\u001b[39;49mtypical_p,\n\u001b[1;32m   1569\u001b[0m         logprobs\u001b[39m=\u001b[39;49mlogprobs,\n\u001b[1;32m   1570\u001b[0m         echo\u001b[39m=\u001b[39;49mecho,\n\u001b[1;32m   1571\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m   1572\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49mfrequency_penalty,\n\u001b[1;32m   1573\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49mpresence_penalty,\n\u001b[1;32m   1574\u001b[0m         repeat_penalty\u001b[39m=\u001b[39;49mrepeat_penalty,\n\u001b[1;32m   1575\u001b[0m         top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m   1576\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1577\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m   1578\u001b[0m         tfs_z\u001b[39m=\u001b[39;49mtfs_z,\n\u001b[1;32m   1579\u001b[0m         mirostat_mode\u001b[39m=\u001b[39;49mmirostat_mode,\n\u001b[1;32m   1580\u001b[0m         mirostat_tau\u001b[39m=\u001b[39;49mmirostat_tau,\n\u001b[1;32m   1581\u001b[0m         mirostat_eta\u001b[39m=\u001b[39;49mmirostat_eta,\n\u001b[1;32m   1582\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1583\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1584\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1585\u001b[0m         grammar\u001b[39m=\u001b[39;49mgrammar,\n\u001b[1;32m   1586\u001b[0m         logit_bias\u001b[39m=\u001b[39;49mlogit_bias,\n\u001b[1;32m   1587\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/llama_cpp/llama.py:1494\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1492\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[39m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1493\u001b[0m     \u001b[39mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1494\u001b[0m completion: Completion \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(completion_or_chunks)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m \u001b[39mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/llama_cpp/llama.py:1019\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1017\u001b[0m finish_reason \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m multibyte_fix \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1019\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m   1020\u001b[0m     prompt_tokens,\n\u001b[1;32m   1021\u001b[0m     top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m   1022\u001b[0m     top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m   1023\u001b[0m     min_p\u001b[39m=\u001b[39mmin_p,\n\u001b[1;32m   1024\u001b[0m     typical_p\u001b[39m=\u001b[39mtypical_p,\n\u001b[1;32m   1025\u001b[0m     temp\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m   1026\u001b[0m     tfs_z\u001b[39m=\u001b[39mtfs_z,\n\u001b[1;32m   1027\u001b[0m     mirostat_mode\u001b[39m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1028\u001b[0m     mirostat_tau\u001b[39m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1029\u001b[0m     mirostat_eta\u001b[39m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1030\u001b[0m     frequency_penalty\u001b[39m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1031\u001b[0m     presence_penalty\u001b[39m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1032\u001b[0m     repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1033\u001b[0m     stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1034\u001b[0m     logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m   1035\u001b[0m     grammar\u001b[39m=\u001b[39mgrammar,\n\u001b[1;32m   1036\u001b[0m ):\n\u001b[1;32m   1037\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m     \u001b[39mif\u001b[39;00m llama_cpp\u001b[39m.\u001b[39mllama_token_is_eog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mmodel, token):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/llama_cpp/llama.py:698\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39m# Eval and sample\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 698\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[1;32m    699\u001b[0m     \u001b[39mwhile\u001b[39;00m sample_idx \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_tokens:\n\u001b[1;32m    700\u001b[0m         token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    701\u001b[0m             top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    702\u001b[0m             top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m             idx\u001b[39m=\u001b[39msample_idx,\n\u001b[1;32m    717\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/llama_cpp/llama.py:536\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    532\u001b[0m n_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch)\n\u001b[1;32m    533\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch\u001b[39m.\u001b[39mset_batch(\n\u001b[1;32m    534\u001b[0m     batch\u001b[39m=\u001b[39mbatch, n_past\u001b[39m=\u001b[39mn_past, logits_all\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_params\u001b[39m.\u001b[39mlogits_all\n\u001b[1;32m    535\u001b[0m )\n\u001b[0;32m--> 536\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ctx\u001b[39m.\u001b[39;49mdecode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch)\n\u001b[1;32m    537\u001b[0m \u001b[39m# Save tokens\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_ids[n_past : n_past \u001b[39m+\u001b[39m n_tokens] \u001b[39m=\u001b[39m batch\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/llama_cpp/_internals.py:311\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39massert\u001b[39;00m batch\u001b[39m.\u001b[39mbatch \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m return_code \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_decode(\n\u001b[1;32m    312\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx,\n\u001b[1;32m    313\u001b[0m     batch\u001b[39m.\u001b[39;49mbatch,\n\u001b[1;32m    314\u001b[0m )\n\u001b[1;32m    315\u001b[0m \u001b[39mif\u001b[39;00m return_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    316\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_decode returned \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "topics, probs = topic_model.fit_transform(documents=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama 3 testing - too slow to run locally\n",
    "acess_token = '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'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", token=acess_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", token=acess_token)\n",
    "\n",
    "generator = pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=500,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "prompt = \"Could you explain to me how 4-bit quantization works as if I am 5?\"\n",
    "res = generator(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da846669c2707f04be04994f937304aa0d48e9c850c251ffb85bee71b03b94e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
